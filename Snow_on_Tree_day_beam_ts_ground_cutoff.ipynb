{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e89d72-e7ac-41ca-8e50-2151edea43ff",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f08b8-0f10-43c9-b4c9-b2b869173961",
   "metadata": {},
   "source": [
    "## Install Python libraries\n",
    "* To temporarily install Python libraries for server use, use the command: `%pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c59d819d-099d-4b0f-a7a5-44547663066a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simplekml in /srv/conda/envs/notebook/lib/python3.11/site-packages (1.3.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: geopy in /srv/conda/envs/notebook/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from geopy) (2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: kaleido in /srv/conda/envs/notebook/lib/python3.11/site-packages (0.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: dask[complete] in /srv/conda/envs/notebook/lib/python3.11/site-packages (2024.12.0)\n",
      "Requirement already satisfied: click>=8.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (24.1)\n",
      "Requirement already satisfied: partd>=1.4.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (0.12.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.13.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (8.0.0)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (17.0.0)\n",
      "Requirement already satisfied: lz4>=4.3.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (4.3.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from importlib_metadata>=4.13.0->dask[complete]) (3.19.2)\n",
      "Requirement already satisfied: locket in /srv/conda/envs/notebook/lib/python3.11/site-packages (from partd>=1.4.0->dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pyarrow>=14.0.1->dask[complete]) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (2.2.2)\n",
      "Requirement already satisfied: dask-expr<1.2,>=1.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (1.1.20)\n",
      "Requirement already satisfied: distributed==2024.12.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (2024.12.0)\n",
      "Requirement already satisfied: bokeh>=3.1.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (3.2.2)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from dask[complete]) (3.1.4)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from distributed==2024.12.0->dask[complete]) (1.0.8)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from distributed==2024.12.0->dask[complete]) (5.9.8)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from distributed==2024.12.0->dask[complete]) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from distributed==2024.12.0->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from distributed==2024.12.0->dask[complete]) (6.4.1)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from distributed==2024.12.0->dask[complete]) (1.26.19)\n",
      "Requirement already satisfied: zict>=3.0.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from distributed==2024.12.0->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from bokeh>=3.1.0->dask[complete]) (1.2.1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from bokeh>=3.1.0->dask[complete]) (10.2.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from bokeh>=3.1.0->dask[complete]) (2024.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from jinja2>=2.10.3->dask[complete]) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pandas>=2.0->dask[complete]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pandas>=2.0->dask[complete]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pandas>=2.0->dask[complete]) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[complete]) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install simplekml\n",
    "%pip install geopy\n",
    "%pip install -U kaleido\n",
    "%pip install --upgrade dask[complete]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f6c2a-d266-45df-8462-661c9f4b37a2",
   "metadata": {},
   "source": [
    "# Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf83b48-f1a0-4075-92cd-da4f86484f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sliderule import sliderule, icesat2, earthdata\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import folium\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, Point, mapping\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import geopy\n",
    "import simplekml\n",
    "from geopy.distance import geodesic\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from scipy.stats import linregress\n",
    "import statsmodels.api as sm\n",
    "import dask.array as da\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "664a4c52-327e-4682-957a-7505c74e8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_shape = (1000, 1000)\n",
    "shape = (1000, 4000)\n",
    "ones = da.ones(shape, chunks=chunk_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1947c2e8-e65b-4d4f-9255-24e71d2af078",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_meter = \"20\"\n",
    "cnt = \"10\"\n",
    "ats = \"10\"\n",
    "\n",
    "site_name = \"BONA\"\n",
    "# site_name = \"DEJU\"\n",
    "# site_name = \"WREF\"\n",
    "# site_name = \"RMNP\"\n",
    "# site_name = \"TEAK\"\n",
    "\n",
    "boundary_km = \"8\"\n",
    "ground_offset = 2\n",
    "\n",
    "year_folder = 'year/'\n",
    "day_folder = 'day/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c764bf8b-b6a6-481f-ab8f-b27d33237c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BONA\n",
      "['2019-01-24', '2019-02-12', '2019-02-26', '2019-03-13', '2019-04-25', '2019-05-28', '2019-06-12', '2019-06-26', '2019-07-15', '2019-08-26', '2019-09-11', '2019-09-24', '2019-11-12', '2019-12-11', '2019-12-24', '2020-01-13', '2020-01-22', '2020-02-11', '2020-02-24', '2020-03-11', '2020-04-22', '2020-05-11', '2020-07-12', '2020-08-10', '2020-09-22', '2020-11-09', '2020-11-23', '2020-12-08', '2021-02-08', '2021-02-21', '2021-03-09', '2021-03-22', '2021-06-21', '2021-07-20', '2021-09-06', '2021-10-19', '2021-11-07', '2022-02-20', '2022-03-07', '2022-03-21', '2022-04-19', '2022-05-21', '2022-06-06', '2022-06-19', '2022-08-07', '2022-08-20', '2022-09-05', '2022-09-18', '2022-10-08', '2022-10-17', '2022-11-06', '2022-11-19', '2022-12-05', '2022-12-18', '2023-01-16', '2023-01-20', '2023-01-31', '2023-02-04', '2023-02-18', '2023-03-05', '2023-03-19', '2023-04-17', '2023-04-21', '2023-05-06', '2023-05-20', '2023-07-17', '2023-08-05', '2023-09-16', '2023-10-15', '2023-11-04', '2023-11-13']\n"
     ]
    }
   ],
   "source": [
    "def compile_dates_for_site(folder, site_name):\n",
    "    \"\"\"\n",
    "    Reads all CSV files for the specified site in a folder, extracts the 'date' column,\n",
    "    and compiles a unique list of dates.\n",
    "\n",
    "    Args:\n",
    "        folder (str): The directory containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of unique dates for the specified site.\n",
    "    \"\"\"\n",
    "    print(site_name)\n",
    "    all_dates = set()  # Use a set to ensure unique dates\n",
    "\n",
    "    # Iterate through all files in the folder\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.startswith(site_name) and filename.endswith(\".csv\"):  # Match site_name in the file name\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            try:\n",
    "                # Read the CSV file into a DataFrame\n",
    "                df = pd.read_csv(filepath)\n",
    "                if 'date' in df.columns and 'ground_photon_count' in df.columns and 'canopy_photon_count' in df.columns:\n",
    "                    # Filter rows where both ground_photon_count and canopy_photon_count are 0\n",
    "                    filtered_df = df[(df['ground_photon_count'] != 0) | (df['canopy_photon_count'] != 0)]\n",
    "                    # Add unique dates directly from the 'date' column as strings\n",
    "                    all_dates.update(filtered_df['date'].dropna().unique())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    # Return sorted list of unique dates\n",
    "    return sorted(all_dates)\n",
    "\n",
    "site_dates = compile_dates_for_site(year_folder, site_name)\n",
    "print(site_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7340d72d-6f9e-4168-b035-29c3be5cd908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-09-05', '2022-09-18', '2022-10-08', '2022-10-17', '2022-11-06', '2022-11-19', '2022-12-05', '2022-12-18', '2023-01-16', '2023-01-20', '2023-01-31', '2023-02-04', '2023-02-18', '2023-03-05', '2023-03-19', '2023-04-17', '2023-04-21', '2023-05-06', '2023-05-20']\n"
     ]
    }
   ],
   "source": [
    "# unit test\n",
    "\n",
    "def filter_dates(site_dates, start_time, end_time):\n",
    "    # Convert start_time and end_time to datetime objects\n",
    "    start_time = datetime.strptime(start_time, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    end_time = datetime.strptime(end_time, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    \n",
    "    # Convert site_dates to datetime objects\n",
    "    site_dates = [datetime.strptime(date, \"%Y-%m-%d\") for date in site_dates]\n",
    "    \n",
    "    # Filter site_dates to include only those within the range\n",
    "    filtered_dates = [date.strftime(\"%Y-%m-%d\") for date in site_dates if start_time <= date <= end_time]\n",
    "    \n",
    "    return filtered_dates\n",
    "\n",
    "start_time = \"2022-09-01T00:00:00Z\"\n",
    "end_time = \"2023-05-31T23:59:59Z\"\n",
    "\n",
    "site_dates = filter_dates(site_dates, start_time, end_time)\n",
    "print(site_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97687054-26d3-49b9-ae5c-e47e006961be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BONA\n",
      "{'date': '2022-09-05', 'start_time': '2022-09-05T00:00:00Z', 'end_time': '2022-09-05T23:59:59Z'}\n",
      "{'date': '2022-09-18', 'start_time': '2022-09-18T00:00:00Z', 'end_time': '2022-09-18T23:59:59Z'}\n",
      "{'date': '2022-10-08', 'start_time': '2022-10-08T00:00:00Z', 'end_time': '2022-10-08T23:59:59Z'}\n",
      "{'date': '2022-10-17', 'start_time': '2022-10-17T00:00:00Z', 'end_time': '2022-10-17T23:59:59Z'}\n",
      "{'date': '2022-11-06', 'start_time': '2022-11-06T00:00:00Z', 'end_time': '2022-11-06T23:59:59Z'}\n",
      "{'date': '2022-11-19', 'start_time': '2022-11-19T00:00:00Z', 'end_time': '2022-11-19T23:59:59Z'}\n",
      "{'date': '2022-12-05', 'start_time': '2022-12-05T00:00:00Z', 'end_time': '2022-12-05T23:59:59Z'}\n",
      "{'date': '2022-12-18', 'start_time': '2022-12-18T00:00:00Z', 'end_time': '2022-12-18T23:59:59Z'}\n",
      "{'date': '2023-01-16', 'start_time': '2023-01-16T00:00:00Z', 'end_time': '2023-01-16T23:59:59Z'}\n",
      "{'date': '2023-01-20', 'start_time': '2023-01-20T00:00:00Z', 'end_time': '2023-01-20T23:59:59Z'}\n",
      "{'date': '2023-01-31', 'start_time': '2023-01-31T00:00:00Z', 'end_time': '2023-01-31T23:59:59Z'}\n",
      "{'date': '2023-02-04', 'start_time': '2023-02-04T00:00:00Z', 'end_time': '2023-02-04T23:59:59Z'}\n",
      "{'date': '2023-02-18', 'start_time': '2023-02-18T00:00:00Z', 'end_time': '2023-02-18T23:59:59Z'}\n",
      "{'date': '2023-03-05', 'start_time': '2023-03-05T00:00:00Z', 'end_time': '2023-03-05T23:59:59Z'}\n",
      "{'date': '2023-03-19', 'start_time': '2023-03-19T00:00:00Z', 'end_time': '2023-03-19T23:59:59Z'}\n",
      "{'date': '2023-04-17', 'start_time': '2023-04-17T00:00:00Z', 'end_time': '2023-04-17T23:59:59Z'}\n",
      "{'date': '2023-04-21', 'start_time': '2023-04-21T00:00:00Z', 'end_time': '2023-04-21T23:59:59Z'}\n",
      "{'date': '2023-05-06', 'start_time': '2023-05-06T00:00:00Z', 'end_time': '2023-05-06T23:59:59Z'}\n",
      "{'date': '2023-05-20', 'start_time': '2023-05-20T00:00:00Z', 'end_time': '2023-05-20T23:59:59Z'}\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "def generate_date_time_ranges(dates):\n",
    "    \"\"\"\n",
    "    Generate a dictionary of start and end times for each unique date.\n",
    "\n",
    "    Args:\n",
    "        dates (list of str): List of date strings in the format \"YYYY-MM-DD\".\n",
    "        \n",
    "    Returns:\n",
    "        list of dict: A list of dictionaries where each dictionary represents a time range\n",
    "                      with 'start_time' and 'end_time' for a specific date.\n",
    "    \"\"\"\n",
    "    time_ranges = []\n",
    "    for date in dates:\n",
    "        start_time = f\"{date}T00:00:00Z\"\n",
    "        end_time = f\"{date}T23:59:59Z\"\n",
    "        \n",
    "        # Append the date with its start_time and end_time to the time_ranges list\n",
    "        time_ranges.append({\n",
    "            \"date\": date, \n",
    "            \"start_time\": start_time, \n",
    "            \"end_time\": end_time\n",
    "        })\n",
    "    return time_ranges\n",
    "\n",
    "site_time_ranges = generate_date_time_ranges(site_dates)\n",
    "print(site_name)\n",
    "for date in site_time_ranges:\n",
    "    print(date)\n",
    "print(len(site_time_ranges))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f280840b-a26d-467c-a6ff-7b9329a5ba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 2022-10-08 because it only contains geometry data.\n",
      "Skipping 2022-12-05 because it only contains geometry data.\n",
      "Skipping 2023-01-16 because it only contains geometry data.\n",
      "Skipping 2023-02-04 because it only contains geometry data.\n",
      "Skipping 2023-05-20 because it only contains geometry data.\n",
      "CPU times: user 47.5 s, sys: 1.3 s, total: 48.8 s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def process_site(site, time_ranges, boundary_km):\n",
    "    \"\"\"\n",
    "    Processes a site by requesting ATL03 data for specified time ranges.\n",
    "\n",
    "    Args:\n",
    "        region (dict): A dictionary containing the spatial region of interest, with a 'poly' key for the polygon.\n",
    "        time_ranges (dict): A dictionary where keys are years, and values are dictionaries with 'start_time' and 'end_time'.\n",
    "        cnt (int, optional): Number of photons required for the analysis. Defaults to None.\n",
    "        ats (int, optional): Along-track spacing for the data. Defaults to None.\n",
    "        segment_meter (float, optional): Segment length and resolution in meters. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are years, and values are the retrieved ATL03 data.\n",
    "    \"\"\"\n",
    "    # Load the region dynamically based on the site\n",
    "    region = sliderule.toregion(f\"geojson_files/{site}_buffer_{boundary_km}km.geojson\")\n",
    "    \n",
    "    # Dictionary to store ATL03 data for each year\n",
    "    site_data = {}\n",
    "    \n",
    "    for time_range  in time_ranges:\n",
    "        # Construct the parameters for the current time range\n",
    "        parms = {\n",
    "            \"poly\": region['poly'],       # Region polygon\n",
    "            \"t0\": time_range['start_time'],   # Start time\n",
    "            \"t1\": time_range['end_time'],     # End time\n",
    "            \"srt\": icesat2.SRT_LAND,           # Surface return type\n",
    "            \"cnf\": 0,                    # Confidence level\n",
    "            \"cnt\": cnt,                  # Number of photons\n",
    "            \"ats\": ats,                  # Along-track spacing\n",
    "            \"len\": segment_meter,        # Segment length\n",
    "            \"res\": segment_meter,        # Resolution\n",
    "            \"atl08_class\": [             # ATL08 classifications\n",
    "                \"atl08_noise\",\n",
    "                \"atl08_ground\",\n",
    "                \"atl08_canopy\",\n",
    "                \"atl08_top_of_canopy\",\n",
    "                \"atl08_unclassified\"\n",
    "            ],\n",
    "            \"phoreal\": {                 # Phoreal processing settings\n",
    "                \"binsize\": 1.0,\n",
    "                \"geoloc\": \"mean\",\n",
    "                \"use_abs_h\": True,\n",
    "                \"send_waveform\": True\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Request the ATL03 data using icesat2\n",
    "        atl03_data = icesat2.atl03sp(parms, keep_id=True)\n",
    "        \n",
    "       # If only the 'geometry' column remains, skip this date\n",
    "        if atl03_data.shape[1] == 1 and 'geometry' in atl03_data.columns:\n",
    "            print(f\"Skipping {time_range['date']} because it only contains geometry data.\")\n",
    "            continue\n",
    "            \n",
    "        columns_to_drop = [\n",
    "        'region', 'pair', 'segment_dist', 'segment_id', 'cycle', 'track', 'background_rate', 'y_atc', \n",
    "            'yapc_score', 'atl03_cnf', 'relief', 'quality_ph'\n",
    "        ]\n",
    "        \n",
    "        # Drop the columns from the DataFrame\n",
    "        atl03_data = atl03_data.drop(columns=columns_to_drop)\n",
    "        \n",
    "        # Use the date as the key for storing data\n",
    "        site_data[time_range['date']] = atl03_data\n",
    "\n",
    "    return site_data\n",
    "\n",
    "site_data = process_site(site_name, site_time_ranges, boundary_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d38f37d-aa74-400c-b377-fe72da806224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Date: 2022-09-05\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 3471\n",
      "Number of Photons Missing Height Data: 0\n",
      "Date: 2022-09-18\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 131153\n",
      "Number of Photons Missing Height Data: 0\n",
      "Date: 2022-10-17\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 2225\n",
      "Number of Photons Missing Height Data: 0\n",
      "Date: 2022-11-06\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 299803\n",
      "Number of Photons Missing Height Data: 0\n",
      "Date: 2022-11-19\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 42\n",
      "Number of Photons Missing Height Data: 0\n",
      "Date: 2022-12-18\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 209109\n",
      "Number of Photons Missing Height Data: 0\n",
      "Date: 2023-01-20\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 39617\n",
      "Number of Photons Missing Height Data: 0\n",
      "Date: 2023-01-31\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 396750\n",
      "Number of Photons Missing Height Data: 0\n",
      "Date: 2023-02-18\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 105717\n",
      "Number of Photons Missing Height Data: 0\n",
      "Date: 2023-03-05\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 659168\n",
      "Number of Photons Missing Height Data: 0\n",
      "Date: 2023-03-19\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 179323\n",
      "Number of Photons Missing Height Data: 0\n",
      "Date: 2023-04-17\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 2551\n",
      "Number of Photons Missing Height Data: 0\n",
      "Date: 2023-04-21\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 89373\n",
      "Number of Photons Missing Height Data: 0\n",
      "Date: 2023-05-06\n",
      "Empty GeoDataFrame\n",
      "Columns: [extent_id, rgt, sc_orient, solar_elevation, height, x_atc, atl08_class, snowcover, landcover, geometry, spot]\n",
      "Index: []\n",
      "Total Photons: 299233\n",
      "Number of Photons Missing Height Data: 0\n"
     ]
    }
   ],
   "source": [
    "# Loop through the site_data dictionary\n",
    "\n",
    "print (len(site_data))\n",
    "for date, site_datum in site_data.items():\n",
    "    print(f\"Date: {date}\")\n",
    "    \n",
    "    # print(\"Columns:\", site_datum.columns.tolist())  # Print the list of column names\n",
    "    \n",
    "    # Get the total number of rows (total photons)\n",
    "    total_photons = site_datum.shape[0]\n",
    "    \n",
    "    # Count how many rows have missing ('NaN') or zero ('0') in the 'height' column\n",
    "    missing_or_zero_height_count = site_datum['height'].isna().sum() + (site_datum['height'] == 0).sum()\n",
    "    # print(site_datum['height'].dtype)\n",
    "    print(site_datum[site_datum['height'].isna()])\n",
    "    # Print total photons (rows) and number of rows missing height data\n",
    "    print(f\"Total Photons: {total_photons}\")\n",
    "    print(f\"Number of Photons Missing Height Data: {missing_or_zero_height_count}\")\n",
    "    \n",
    "    # print(site_datum.head())  # Print the list of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0deb709c-83e1-48b3-8f6c-1fa43b59be89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 µs, sys: 0 ns, total: 11 µs\n",
      "Wall time: 15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Group by 'extent_id' and 'x_atc' to define each photon group\n",
    "def get_photons_per_set(df, distance_window=5):\n",
    "    \"\"\"\n",
    "    Group data by 'x_atc' distance window, creating sets based on distance (e.g., 5 meters per window) \n",
    "    and considering unique 'extent_id'.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(by=['extent_id', 'x_atc'])  # Ensure sorted by extent and along-track distance\n",
    "    df['distance_group'] = df.groupby('extent_id')['x_atc'].transform(lambda x: (x // distance_window).astype(int))\n",
    "    return df\n",
    "\n",
    "# Apply rolling window (moving average) for ground height data\n",
    "def moving_window_distance(df, window_size=3, min_photons=2):\n",
    "    \"\"\"\n",
    "    Calculate average ground height using a moving window on 'distance_group', ensuring enough ground photons.\n",
    "    \"\"\"\n",
    "    # Save the index to a new column 'time'\n",
    "    df['time'] = df.index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Filter for ground photons (atl08_class == 1)\n",
    "    ground_photons = df[df['atl08_class'] == 1].copy()\n",
    "    \n",
    "    # Apply rolling average calculation for ground heights\n",
    "    ground_photons['avg_ground_height'] = ground_photons.groupby(['extent_id', 'distance_group'])['height'].transform(\n",
    "        lambda x: x.rolling(window=window_size, min_periods=min_photons).mean()\n",
    "    )\n",
    "    \n",
    "    # Handle groups with insufficient photons\n",
    "    ground_photons['avg_ground_height'] = ground_photons.groupby(['extent_id', 'distance_group'])['height'].transform(\n",
    "        lambda x: np.nan if len(x) < min_photons else x.mean()\n",
    "    )\n",
    "    \n",
    "    # Fill NaN values using forward and backward fills for ground photons only\n",
    "    ground_photons['avg_ground_height'] = ground_photons['avg_ground_height'].ffill().bfill()\n",
    "    \n",
    "    # Now assign 'avg_ground_height' back to the original df for only the rows that were ground photons\n",
    "    df.loc[df['atl08_class'] == 1, 'avg_ground_height'] = ground_photons['avg_ground_height']\n",
    "\n",
    "    # Reset the index back to 'time'\n",
    "    df = df.set_index('time')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Compute relative height by subtracting average ground height\n",
    "def calculate_relative_height(df, ground_offset = 2):\n",
    "    \"\"\"\n",
    "    Subtract average ground height from each photon to compute relative height.\n",
    "    \"\"\"\n",
    "    df['time'] = df.index\n",
    "    \n",
    "    # Separate canopy (class 2 or 3) and ground (class 1) photons\n",
    "    canopy_photons = df[df['atl08_class'].isin([2, 3])].copy()\n",
    "    ground_photons = df[df['atl08_class'] == 1].copy()\n",
    "\n",
    "    # Try to find the first non-zero and non-NaN avg_ground_height\n",
    "    valid_ground_heights = ground_photons['avg_ground_height'].dropna().loc[ground_photons['avg_ground_height'] != 0]\n",
    "\n",
    "    if len(valid_ground_heights) > 0:\n",
    "        avg_ground_height = valid_ground_heights.iloc[0]  # Take the first valid value\n",
    "    else:\n",
    "        avg_ground_height = None  # No valid avg_ground_height found\n",
    "    \n",
    "    # If avg_ground_height is None, set the relative height for canopy photons to NaN or 0 (your choice)\n",
    "    if avg_ground_height is None:\n",
    "        canopy_photons['relative_height'] = 0  # use 0 or None if you prefer\n",
    "    else:\n",
    "        # Compute relative height for canopy photons (subtract the average ground height)\n",
    "        canopy_photons['relative_height'] = canopy_photons['height'] - avg_ground_height\n",
    "        canopy_photons['relative_height'] = canopy_photons['relative_height'].fillna(0)  # Replace NaNs with 0\n",
    "\n",
    "    # Now, handle the ground photons by assigning them a relative height of 0 (since they are at ground level)\n",
    "    ground_photons['relative_height'] = 0  # Ground photons' relative height is 0\n",
    "    \n",
    "    # Check for any values that are not NaN or 0 in the 'relative_height' column\n",
    "    non_zero_values = ground_photons['relative_height'].isna() | (ground_photons['relative_height'] != 0)\n",
    "\n",
    "    # Count photons in class 2 or 3 before modification\n",
    "    initial_count = canopy_photons.shape[0]\n",
    "\n",
    "    # Count the canopy photons that satisfy the condition (relative_height <= ground_offset)\n",
    "    moved_count = canopy_photons[canopy_photons['relative_height'] <= ground_offset].shape[0]\n",
    "\n",
    "    # Reassign canopy photons (atl08_class 2 or 3) with height <= 1 to atl08_class 1 (ground)\n",
    "    canopy_photons.loc[canopy_photons['relative_height'] <= ground_offset, 'atl08_class'] = 1\n",
    "    canopy_photons.loc[canopy_photons['relative_height'] >= 30, 'atl08_class'] = 0 # above 30 height reassigned to noise\n",
    "    canopy_photons.loc[canopy_photons['atl08_class'] == 1, 'relative_height'] = 0  # Set relative height to 0 for reassigned canopy photons\n",
    "\n",
    "    # Combine canopy and ground photons back together\n",
    "    df = pd.concat([canopy_photons, ground_photons], ignore_index=False)\n",
    "    # Reset the index back to 'time'\n",
    "    df = df.set_index('time')\n",
    "    \n",
    "    return df, initial_count, moved_count\n",
    "\n",
    "def log_reassignment_data(date, date_init_count, date_moved_count, extents, folder, ground_offset, site_name, log_file_name=\"canopy_reassignment.log\"):\n",
    "    \"\"\"\n",
    "    Function to log reassignment data (including photon count and min/max heights) and summary to a .log file.\n",
    "    \n",
    "    Parameters:\n",
    "    date (str): Date for the current data.\n",
    "    date_init_count (int): Total number of initial photons.\n",
    "    date_moved_count (int): Total number of photons reassigned to ground.\n",
    "    extents (list): List of extents with relevant data for logging.\n",
    "    folder (str): Folder where the log file will be saved.\n",
    "    log_file_name (str): Name of the log file to save the output to (default is 'extents_log.log').\n",
    "    \"\"\"\n",
    "    # Ensure folder exists\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    log_file_path = os.path.join(folder, site_name, log_file_name)\n",
    "    \n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        # Write header\n",
    "        log_file.write(\"Extent Data Log\\n\")\n",
    "        log_file.write(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Write summary for the date\n",
    "        log_file.write(f\"Site Name: {site_name}\\n\")\n",
    "        log_file.write(f\"Date: {date}\\n\")\n",
    "        log_file.write(f\"Ground Offset: {ground_offset}\\n\")\n",
    "        log_file.write(f\"Total Photons: {date_init_count}\\n\")\n",
    "        log_file.write(f\"Total Photons Reassigned to Ground: {date_moved_count}\\n\")\n",
    "        log_file.write(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # Randomly sample 10 extents if there are enough\n",
    "        sampled_extents = random.sample(extents, min(10, len(extents)))\n",
    "\n",
    "        for extent in sampled_extents:\n",
    "            log_file.write(f\"Extent ID: {extent['extent_id']}\\n\")\n",
    "            log_file.write(f\"  Canopy Photons: {extent['canopy_photon_count']}\\n\")\n",
    "            log_file.write(f\"  Canopy Min Height: {extent['canopy_min_ht']}\\n\")\n",
    "            log_file.write(f\"  Canopy Max Height: {extent['canopy_max_ht']}\\n\")\n",
    "            log_file.write(f\"  Ground Min Height: {extent['ground_min_ht']}\\n\")\n",
    "            log_file.write(f\"  Ground Max Height: {extent['ground_max_ht']}\\n\")\n",
    "            log_file.write(f\"  Canopy Min Relative Height: {extent['canopy_min_rel_ht']}\\n\")\n",
    "            log_file.write(f\"  Canopy Max Relative Height: {extent['canopy_max_rel_ht']}\\n\")\n",
    "            log_file.write(f\"  Ground Min Relative Height: {extent['ground_min_rel_ht']}\\n\")\n",
    "            log_file.write(f\"  Ground Max Relative Height: {extent['ground_max_rel_ht']}\\n\")\n",
    "            log_file.write(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ce83693-18e6-4205-b51b-0b69586b3cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Date: 2022-09-05\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n",
      "Date: 2022-09-18\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n",
      "Date: 2022-10-17\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n",
      "Date: 2022-11-06\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n",
      "Date: 2022-11-19\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n",
      "Date: 2022-12-18\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n",
      "Date: 2023-01-20\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n",
      "Date: 2023-01-31\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n",
      "Date: 2023-02-18\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n",
      "Date: 2023-03-05\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n",
      "Date: 2023-03-19\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n",
      "Date: 2023-04-17\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n",
      "Date: 2023-04-21\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n",
      "Date: 2023-05-06\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot']\n"
     ]
    }
   ],
   "source": [
    "# Loop through the site_data dictionary\n",
    "\n",
    "print (len(site_data))\n",
    "for date, site_datum in site_data.items():\n",
    "    print(f\"Date: {date}\")\n",
    "    print(\"Columns:\", site_datum.columns.tolist())  # Print the list of column names\n",
    "    # print(site_datum.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "999fc3f7-e173-42fb-88a5-eff54df0143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Extents for 2022-09-05: 100%|██████████| 147/147 [00:02<00:00, 60.21it/s]\n",
      "Processing Extents for 2022-09-18: 100%|██████████| 1560/1560 [00:25<00:00, 60.76it/s]\n",
      "Processing Extents for 2022-10-17: 100%|██████████| 69/69 [00:01<00:00, 63.56it/s]\n",
      "Processing Extents for 2022-11-06: 100%|██████████| 2393/2393 [00:43<00:00, 55.21it/s]\n",
      "Processing Extents for 2022-11-19: 100%|██████████| 3/3 [00:00<00:00, 71.68it/s]\n",
      "Processing Extents for 2022-12-18: 100%|██████████| 1589/1589 [00:28<00:00, 55.97it/s]\n",
      "Processing Extents for 2023-01-20: 100%|██████████| 731/731 [00:12<00:00, 58.83it/s]\n",
      "Processing Extents for 2023-01-31: 100%|██████████| 2040/2040 [00:33<00:00, 60.06it/s]\n",
      "Processing Extents for 2023-02-18: 100%|██████████| 804/804 [00:14<00:00, 56.04it/s]\n",
      "Processing Extents for 2023-03-05: 100%|██████████| 1827/1827 [00:28<00:00, 64.24it/s]\n",
      "Processing Extents for 2023-03-19: 100%|██████████| 1605/1605 [00:28<00:00, 55.86it/s]\n",
      "Processing Extents for 2023-04-17: 100%|██████████| 119/119 [00:01<00:00, 60.60it/s]\n",
      "Processing Extents for 2023-04-21: 100%|██████████| 804/804 [00:14<00:00, 56.42it/s]\n",
      "Processing Extents for 2023-05-06: 100%|██████████| 800/800 [00:14<00:00, 56.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 11s, sys: 732 ms, total: 4min 12s\n",
      "Wall time: 4min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def process_extents_per_date(site_data):\n",
    "    \"\"\"\n",
    "    Processes the ATL03 data for each extent per date and returns a dictionary of processed data.\n",
    "\n",
    "    Args:\n",
    "        site_data (dict): A dictionary where keys are date strings and values are ATL03 DataFrames.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are date strings and values are processed ATL03 DataFrames.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store processed data\n",
    "    processed_data_dict = {}\n",
    "    height_stats_dict = {}\n",
    "\n",
    "    # Loop through each date in the site_data\n",
    "    for date, atl03_data in site_data.items():\n",
    "        # Initialize an empty list to collect processed data for the current date\n",
    "        processed_data_list = []\n",
    "        date_init_count = 0\n",
    "        date_moved_count = 0\n",
    "        canopy_min_heights = []  # To store min heights for canopy photons\n",
    "        canopy_max_heights = []  # To store max heights for canopy photons\n",
    "        ground_min_heights = []  # To store min heights for ground photons\n",
    "        ground_max_heights = []  # To store max heights for ground photons\n",
    "        extents = []  # To store data for each extent\n",
    "\n",
    "        # Process each extent_id within the ATL03 data\n",
    "        for extent_id in tqdm(atl03_data['extent_id'].unique(), desc=f\"Processing Extents for {date}\"):\n",
    "            extent_data = atl03_data[atl03_data['extent_id'] == extent_id]\n",
    "\n",
    "            # Process segment data\n",
    "            extent_data = get_photons_per_set(extent_data, distance_window=5)  # 5-meter window\n",
    "            extent_data = moving_window_distance(extent_data, window_size=3, min_photons=2)\n",
    "            extent_data, initial_count, moved_count = calculate_relative_height(extent_data, ground_offset=ground_offset)\n",
    "\n",
    "            # Separate canopy and ground photons\n",
    "            canopy_photons = extent_data[extent_data['atl08_class'].isin([2, 3])]\n",
    "            ground_photons = extent_data[extent_data['atl08_class'] == 1]\n",
    "\n",
    "            # Count the number of canopy photons (class 2 or 3)\n",
    "            canopy_photon_count = canopy_photons.shape[0]\n",
    "\n",
    "            # Calculate min and max heights for canopy and ground photons\n",
    "            canopy_min_ht = canopy_photons['height'].min() if not canopy_photons.empty else None\n",
    "            canopy_max_ht = canopy_photons['height'].max() if not canopy_photons.empty else None\n",
    "            ground_min_ht = ground_photons['height'].min() if not ground_photons.empty else None\n",
    "            ground_max_ht = ground_photons['height'].max() if not ground_photons.empty else None\n",
    "\n",
    "            # Calculate the min/max relative heights (relative_height column) for canopy and ground photons\n",
    "            canopy_min_rel_ht = canopy_photons['relative_height'].min() if not canopy_photons.empty else None\n",
    "            canopy_max_rel_ht = canopy_photons['relative_height'].max() if not canopy_photons.empty else None\n",
    "            ground_min_rel_ht = ground_photons['relative_height'].min() if not ground_photons.empty else None\n",
    "            ground_max_rel_ht = ground_photons['relative_height'].max() if not ground_photons.empty else None\n",
    "\n",
    "            # Append the min/max heights to the respective lists\n",
    "            if canopy_min_ht is not None: canopy_min_heights.append(canopy_min_ht)\n",
    "            if canopy_max_ht is not None: canopy_max_heights.append(canopy_max_ht)\n",
    "            if ground_min_ht is not None: ground_min_heights.append(ground_min_ht)\n",
    "            if ground_max_ht is not None: ground_max_heights.append(ground_max_ht)\n",
    "            \n",
    "            # Append processed data to the list\n",
    "            processed_data_list.append(extent_data)\n",
    "\n",
    "            # Append the min/max relative heights to the list for the current extent\n",
    "            extents.append({\n",
    "                'extent_id': extent_id,\n",
    "                'canopy_photon_count': canopy_photon_count,\n",
    "                'canopy_min_ht': canopy_min_ht,\n",
    "                'canopy_max_ht': canopy_max_ht,\n",
    "                'ground_min_ht': ground_min_ht,\n",
    "                'ground_max_ht': ground_max_ht,\n",
    "                'canopy_min_rel_ht': canopy_min_rel_ht,\n",
    "                'canopy_max_rel_ht': canopy_max_rel_ht,\n",
    "                'ground_min_rel_ht': ground_min_rel_ht,\n",
    "                'ground_max_rel_ht': ground_max_rel_ht\n",
    "            })\n",
    "            \n",
    "            date_init_count = date_init_count + initial_count\n",
    "            date_moved_count = date_moved_count + moved_count\n",
    "\n",
    "        # Randomly sample 10 extents if there are enough\n",
    "        sampled_extents = random.sample(extents, min(10, len(extents)))\n",
    "\n",
    "        log_reassignment_data(date, date_init_count, date_moved_count, extents, day_folder, ground_offset, site_name)\n",
    "        \n",
    "        # Combine processed data for the current date\n",
    "        combined_data = pd.concat(processed_data_list)\n",
    "        # print(len(combined_data))\n",
    "        processed_data_dict[date] = combined_data\n",
    "        \n",
    "    return processed_data_dict\n",
    "\n",
    "# Example usage\n",
    "site_data_with_height = process_extents_per_date(site_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "550b0914-fd3f-4d98-a5a7-bfdbba163118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Date: 2022-09-05\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n",
      "Date: 2022-09-18\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n",
      "Date: 2022-10-17\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n",
      "Date: 2022-11-06\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n",
      "Date: 2022-11-19\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n",
      "Date: 2022-12-18\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n",
      "Date: 2023-01-20\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n",
      "Date: 2023-01-31\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n",
      "Date: 2023-02-18\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n",
      "Date: 2023-03-05\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n",
      "Date: 2023-03-19\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n",
      "Date: 2023-04-17\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n",
      "Date: 2023-04-21\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n",
      "Date: 2023-05-06\n",
      "Columns: ['extent_id', 'rgt', 'sc_orient', 'solar_elevation', 'height', 'x_atc', 'atl08_class', 'snowcover', 'landcover', 'geometry', 'spot', 'distance_group', 'avg_ground_height', 'relative_height']\n"
     ]
    }
   ],
   "source": [
    "# Loop through the site_data dictionary\n",
    "\n",
    "print (len(site_data_with_height))\n",
    "for date, site_datum in site_data_with_height.items():\n",
    "    print(f\"Date: {date}\")\n",
    "    print(\"Columns:\", site_datum.columns.tolist())  # Print the list of column names\n",
    "    # print(site_datum.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76e351fd-78f0-48b9-a45e-cdedbc5d3813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2022-09-05 - Total photons: 2783, Photons with relative_height <= 1 meter: 2266\n",
      "Date: 2022-09-18 - Total photons: 33021, Photons with relative_height <= 1 meter: 24376\n",
      "Date: 2022-10-17 - Total photons: 1651, Photons with relative_height <= 1 meter: 1383\n",
      "Date: 2022-11-06 - Total photons: 242935, Photons with relative_height <= 1 meter: 196374\n",
      "Date: 2022-11-19 - Total photons: 0, Photons with relative_height <= 1 meter: 0\n",
      "Date: 2022-12-18 - Total photons: 164823, Photons with relative_height <= 1 meter: 126898\n",
      "Date: 2023-01-20 - Total photons: 32182, Photons with relative_height <= 1 meter: 24232\n",
      "Date: 2023-01-31 - Total photons: 54724, Photons with relative_height <= 1 meter: 45111\n",
      "Date: 2023-02-18 - Total photons: 81722, Photons with relative_height <= 1 meter: 61144\n",
      "Date: 2023-03-05 - Total photons: 13103, Photons with relative_height <= 1 meter: 10072\n",
      "Date: 2023-03-19 - Total photons: 143065, Photons with relative_height <= 1 meter: 117430\n",
      "Date: 2023-04-17 - Total photons: 2072, Photons with relative_height <= 1 meter: 1590\n",
      "Date: 2023-04-21 - Total photons: 69774, Photons with relative_height <= 1 meter: 55083\n",
      "Date: 2023-05-06 - Total photons: 54436, Photons with relative_height <= 1 meter: 41171\n"
     ]
    }
   ],
   "source": [
    "def count_photons_below_1m(site_data):\n",
    "    \"\"\"\n",
    "    Count the number of photons with relative_height <= 1 meter and total photon count for each date in the site_data.\n",
    "    \n",
    "    Args:\n",
    "        site_data (dict): A dictionary where keys are date strings and values are processed ATL03 DataFrames.\n",
    "    \"\"\"\n",
    "    # Loop through each date in the site_data\n",
    "    for date, atl03_data in site_data.items():\n",
    "        # Count the number of photons with relative_height <= 1 meter\n",
    "        count_below_1m = (atl03_data['relative_height'] <= 1).sum()\n",
    "\n",
    "        # Get the total photon count for the current date\n",
    "        total_photons = atl03_data.shape[0]  # Total number of rows (photons)\n",
    "\n",
    "        # Output the counts for the current date\n",
    "        print(f\"Date: {date} - Total photons: {total_photons}, Photons with relative_height <= 1 meter: {count_below_1m}\")\n",
    "\n",
    "# Example usage after processing\n",
    "count_photons_below_1m(site_data_with_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b55fbfa9-c31a-4528-b202-3eb502e250d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 s, sys: 334 ms, total: 21.7 s\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def process_photons(site_data):\n",
    "    \"\"\"\n",
    "    Processes site data to calculate canopy and terrain photon rates for each segment and checks for duplicate times.\n",
    "\n",
    "    Args:\n",
    "        site_data (dict): A dictionary where each value is a DataFrame containing data for a site.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the same keys as `site_data`, containing processed DataFrames.\n",
    "    \"\"\"\n",
    "    processed_data = {}\n",
    "\n",
    "    for date, data in site_data.items():\n",
    "        # Ensure 'time' is the index and in datetime format\n",
    "        data.index = pd.to_datetime(data.index)\n",
    "\n",
    "        # Extract latitude and longitude from the 'geometry' column\n",
    "        data['latitude'] = data['geometry'].apply(lambda x: x.y if x else None)\n",
    "        data['longitude'] = data['geometry'].apply(lambda x: x.x if x else None)\n",
    "\n",
    "        # Separate photons into canopy and terrain classes\n",
    "        canopy_photons = data[data['atl08_class'].isin([2, 3])]  # Classes 2 and 3 for canopy\n",
    "        terrain_photons = data[data['atl08_class'] == 1]         # Class 1 for ground\n",
    "\n",
    "        # Count photons for each segment\n",
    "        canopy_counts = canopy_photons.groupby('extent_id').size().reset_index(name='canopy_photon_count')\n",
    "        terrain_counts = terrain_photons.groupby('extent_id').size().reset_index(name='terrain_photon_count')\n",
    "\n",
    "        # Convert the index to a column temporarily\n",
    "        data['time_index'] = data.index\n",
    "        \n",
    "        # Group by 'extent_id' and count unique 'time_index' values (representing unique times)\n",
    "        unique_time_count = (\n",
    "            data.groupby('extent_id')['time_index']\n",
    "            .nunique()\n",
    "            .reset_index(name='unique_shots')\n",
    "        )\n",
    "\n",
    "        # Merge canopy and terrain counts into one DataFrame\n",
    "        segment_counts = (\n",
    "            canopy_counts\n",
    "            .merge(terrain_counts, on='extent_id', how='outer')\n",
    "            .merge(unique_time_count, on='extent_id', how='outer')\n",
    "        )\n",
    "        segment_counts['canopy_photon_count'].fillna(0, inplace=True)\n",
    "        segment_counts['terrain_photon_count'].fillna(0, inplace=True)\n",
    "\n",
    "        # Replace any zero or NaN values in 'unique_shots' with NaN to avoid division by zero errors\n",
    "        segment_counts['unique_shots'] = segment_counts['unique_shots'].replace(0, np.nan)\n",
    "\n",
    "        # Calculate photon rates by normalizing with the number of unique shots\n",
    "        segment_counts['canopy_photon_rate'] = segment_counts['canopy_photon_count'] / segment_counts['unique_shots']\n",
    "        segment_counts['terrain_photon_rate'] = segment_counts['terrain_photon_count'] / segment_counts['unique_shots']\n",
    "\n",
    "        # Reset index to keep 'time' as a column for merging\n",
    "        data = data.reset_index()\n",
    "\n",
    "        # Merge photon rates into the main DataFrame\n",
    "        data = data.merge(\n",
    "            segment_counts[['extent_id', 'canopy_photon_rate', 'terrain_photon_rate']], \n",
    "            on='extent_id', how='left'\n",
    "        )\n",
    "\n",
    "        # Check for duplicates in the 'time' column\n",
    "        duplicate_times = data[data.index.duplicated()]\n",
    "\n",
    "        # Log duplicates if needed (optional)\n",
    "        if not duplicate_times.empty:\n",
    "            print(f\"date {date}: Found duplicate times in the 'time' column.\")\n",
    "\n",
    "        # Restore 'time' as the index\n",
    "        data.set_index('time_index', inplace=True)\n",
    "        \n",
    "        # Store processed data\n",
    "        processed_data[date] = data\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "processed_site_data = process_photons(site_data_with_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23e9d740-3cc0-4bb1-ad6e-75bdf24ddb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def output_photon_counts_by_day(processed_site_data, output_dir, boundary_km):\n",
    "    \"\"\"\n",
    "    Outputs photon counts by day for each site, separated by ATL08 categories, into CSV files.\n",
    "\n",
    "    Args:\n",
    "        processed_site_data (dict): A dictionary where each value is a DataFrame containing processed data for a site.\n",
    "        output_dir (str): The directory where the CSV files should be saved.\n",
    "        site_name (str): The name of the site (used for output file naming).\n",
    "        ats (str): The ATS value (used for output file naming).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define ATL08 classification categories and their values\n",
    "    atl08_categories = {\n",
    "        \"unclassified\": 0,\n",
    "        \"ground\": 1,\n",
    "        \"canopy\": 2,\n",
    "        \"top_of_canopy\": 3,\n",
    "        \"noise\": 4\n",
    "    }\n",
    "        \n",
    "    # Dictionary to track open file handles for each year\n",
    "    file_handles = {}\n",
    "    \n",
    "    try:\n",
    "        for date_str, data in processed_site_data.items():\n",
    "            # Parse the year from the date string\n",
    "            year = pd.to_datetime(date_str).year\n",
    "\n",
    "            # Ensure 'date' column is included\n",
    "            data['date'] = pd.to_datetime(date_str).strftime('%Y-%m-%d')\n",
    "\n",
    "            # Count photons for each ATL08 category\n",
    "            category_counts = {}\n",
    "            for category, value in atl08_categories.items():\n",
    "                category_counts[category] = (data[data['atl08_class'] == value].groupby('date').size().reset_index(name=f\"{category}_photon_count\"))\n",
    "\n",
    "            # Count unique segments (extent_id) for each date\n",
    "            segment_counts_by_date = (data.groupby('date')['extent_id'].nunique().reset_index(name=\"segment_count\"))\n",
    "    \n",
    "            # Start with the segment counts\n",
    "            counts_by_date = segment_counts_by_date\n",
    "    \n",
    "            # Merge counts for each ATL08 category\n",
    "            for category, count_df in category_counts.items():\n",
    "                counts_by_date = counts_by_date.merge(count_df, on=\"date\", how=\"outer\")\n",
    "\n",
    "            # Add the total photon count column to counts_by_date\n",
    "            counts_by_date['total_photon_count'] = data.shape[0]\n",
    "            \n",
    "            # Count photons that don't fall into any of the specified ATL08 categories (other category)\n",
    "            # Create a mask for photons that don't belong to any of the specified categories (ground, canopy, top_of_canopy, noise)\n",
    "            other_photons_mask = ~data['atl08_class'].isin([1, 2, 3, 4])  # Exclude ground, canopy, top_of_canopy, noise\n",
    "            other_photons_count = data[other_photons_mask].shape[0]\n",
    "            # Add other photons count\n",
    "            counts_by_date['other_photons'] = other_photons_count\n",
    "    \n",
    "            # Fill NaN values with 0 for days where no photons are present in a category\n",
    "            counts_by_date.fillna(0, inplace=True)\n",
    "            \n",
    "            savepath = os.path.join(output_dir, site_name)\n",
    "            # Ensure the output directory exists\n",
    "            if not os.path.exists(savepath):\n",
    "                os.makedirs(savepath)\n",
    "                \n",
    "            # If the file for this year hasn't been initialized, create it with headers\n",
    "            if year not in file_handles:\n",
    "                savefile = os.path.join(output_dir, site_name, f'{site_name}_photons_{year}_ats{ats}_{boundary_km}km.csv')\n",
    "                file_handles[year] = savefile\n",
    "                counts_by_date.to_csv(savefile, index=False)\n",
    "            else:\n",
    "                # Append to the existing file for the year\n",
    "                counts_by_date.to_csv(file_handles[year], index=False, mode='a', header=False)\n",
    "\n",
    "    finally:\n",
    "        # Close file handles if necessary (pandas handles this implicitly, but good practice)\n",
    "        file_handles.clear()\n",
    "\n",
    "output_photon_counts_by_day(processed_site_data, day_folder, boundary_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6b0f981-30aa-4c5c-8941-07d013410bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Dates:\n",
      "2022-09-05\n",
      "2022-09-18\n",
      "2022-10-17\n",
      "2022-11-06\n",
      "2022-11-19\n",
      "2022-12-18\n",
      "2023-01-20\n",
      "2023-01-31\n",
      "2023-02-18\n",
      "2023-03-05\n",
      "2023-03-19\n",
      "2023-04-17\n",
      "2023-04-21\n",
      "2023-05-06\n",
      "1603\n",
      "                                                       time            extent_id   rgt  sc_orient  solar_elevation      height     x_atc  atl08_class  snowcover  landcover                     geometry  spot  distance_group  avg_ground_height  relative_height   latitude   longitude  canopy_photon_rate  terrain_photon_rate        date\n",
      "time_index                                                                                                                                                                                                                                                                                                                                    \n",
      "2023-03-19 08:29:51.673255424 2023-03-19 08:29:51.673255424  6106882354213552128  1356          0       -23.488976  385.250671 -9.361534            3          2        116   POINT (-147.44863 65.1899)     1              -2                NaN         2.744629  65.189902 -147.448635            0.392857             4.857143  2023-03-19\n",
      "2023-03-19 08:29:51.676055552 2023-03-19 08:29:51.676055552  6106882354213552132  1356          0       -23.489132  380.355560 -9.480679            1          2        116  POINT (-147.44868 65.18972)     1              -2                NaN         0.000000  65.189725 -147.448679            0.250000             4.464286  2023-03-19\n",
      "2023-03-19 08:29:51.678855424 2023-03-19 08:29:51.678855424  6106882354213552136  1356          0       -23.489288  379.578186 -9.602277            3          2        116  POINT (-147.44873 65.18955)     1              -2                NaN         5.001801  65.189547 -147.448725            0.464286             4.750000  2023-03-19\n",
      "2023-03-19 08:29:51.681655296 2023-03-19 08:29:51.681655296  6106882354213552140  1356          0       -23.489450  373.648712 -9.681900            1          2        115  POINT (-147.44877 65.18937)     1              -2                NaN         0.000000  65.189370 -147.448770            0.185185             3.703704  2023-03-19\n",
      "2023-03-19 08:29:51.684455424 2023-03-19 08:29:51.684455424  6106882354213552144  1356          0       -23.489601  370.676910 -9.743486            1          2        115  POINT (-147.44882 65.18919)     1              -2                NaN         0.000000  65.189192 -147.448816            0.142857             4.357143  2023-03-19\n"
     ]
    }
   ],
   "source": [
    "# Adjust display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)        # Use full width of the terminal\n",
    "pd.set_option('display.expand_frame_repr', False)  # Avoid column wrapping\n",
    "\n",
    "# Get the dates from the processed_site_data keys\n",
    "dates = list(site_data_with_height.keys())\n",
    "\n",
    "# Display the dates\n",
    "print(\"Unique Dates:\")\n",
    "for date in dates:\n",
    "    print(date)\n",
    "\n",
    "# Group by 'extent_id' and 'spot', then get the first row for each unique combination\n",
    "unique_spot_rows = processed_site_data[\"2023-03-19\"].drop_duplicates(subset=['extent_id', 'spot'])\n",
    "\n",
    "# Display the result\n",
    "print(len(unique_spot_rows))\n",
    "# unique_spot_rows.head(50)\n",
    "print(unique_spot_rows.head())\n",
    "del unique_spot_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eac9ae74-e8b5-4011-a423-e8ee3fd87bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.61 s, sys: 16 ms, total: 2.62 s\n",
      "Wall time: 2.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def add_landcover_and_date_info(processed_site_data):\n",
    "    \"\"\"\n",
    "    Adds landcover mode and date-related columns (month, year, day) to each site DataFrame in processed_site_data.\n",
    "\n",
    "    Args:\n",
    "        processed_site_data (dict): A dictionary where each value is a DataFrame containing data for a site.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the same keys as `processed_site_data`, containing updated DataFrames.\n",
    "    \"\"\"\n",
    "    valid_landcover_codes = {111, 113, 112, 114, 115, 116, 121, 123, 122, 124, 125, 126, 20}\n",
    "\n",
    "    updated_data = {}\n",
    "\n",
    "    for date, data in processed_site_data.items():\n",
    "        # Step 1: Ensure there are no invalid landcover values before grouping\n",
    "        valid_landcover_data = data[data[\"landcover\"].isin(valid_landcover_codes)].copy()\n",
    "\n",
    "        # Step 2: Calculate the landcover mode for each extent_id\n",
    "        landcover_modes = valid_landcover_data.groupby(\"extent_id\")[\"landcover\"].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "\n",
    "        # Step 3: Assign the calculated mode back to the original DataFrame\n",
    "        data[\"landcover_mode\"] = data[\"extent_id\"].map(landcover_modes)\n",
    "        # print(data.head())\n",
    "\n",
    "        data[\"date\"] = data.index\n",
    "        # Step 4: Ensure the 'time' column is present and in datetime format\n",
    "        if \"date\" in data.columns:\n",
    "            try:\n",
    "                data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "                data.set_index(\"date\", inplace=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting 'time' column to datetime for date {date}: {e}\")\n",
    "                continue  # Skip this date if conversion fails\n",
    "        else:\n",
    "            print(f\"'time' column is missing for date {date}. Skipping.\")\n",
    "            continue  # Skip this date if the 'time' column is missing\n",
    "\n",
    "        # Step 5: Extract month, year, and day for aggregation\n",
    "        data[\"month\"] = data.index.month\n",
    "        data[\"year\"] = data.index.year\n",
    "        data[\"day\"] = data.index.day\n",
    "\n",
    "        # Store the updated DataFrame in the dictionary\n",
    "        updated_data[date] = data\n",
    "\n",
    "    return updated_data\n",
    "\n",
    "new_processed_site_data = add_landcover_and_date_info(processed_site_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c2494e4-0cc6-4bdc-af29-4d6a617d7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust display options\n",
    "# pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# pd.set_option('display.width', None)        # Use full width of the terminal\n",
    "# pd.set_option('display.expand_frame_repr', False)  # Avoid column wrapping\n",
    "\n",
    "# # Group by 'extent_id' and 'spot', then get the first row for each unique combination\n",
    "# unique_spot_rows = processed_site_data[\"2019-09-11\"].drop_duplicates(subset=['extent_id', 'spot'])\n",
    "\n",
    "# # Display the result\n",
    "# print(len(unique_spot_rows))\n",
    "# # unique_spot_rows.head(50)\n",
    "# print(unique_spot_rows.head(50))\n",
    "# del unique_spot_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f716d8b-7cd2-4f29-a7ef-4d03aa65df7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date: 2022-09-05, Daytime_photon_count: 0, Nighttime_photon_count: 2783\n",
      "date: 2022-09-18, Daytime_photon_count: 33021, Nighttime_photon_count: 0\n",
      "date: 2022-10-17, Daytime_photon_count: 0, Nighttime_photon_count: 1651\n",
      "date: 2022-11-06, Daytime_photon_count: 0, Nighttime_photon_count: 242935\n",
      "date: 2022-11-19, Daytime_photon_count: 0, Nighttime_photon_count: 0\n",
      "date: 2022-12-18, Daytime_photon_count: 0, Nighttime_photon_count: 164823\n",
      "date: 2023-01-20, Daytime_photon_count: 0, Nighttime_photon_count: 32182\n",
      "date: 2023-01-31, Daytime_photon_count: 54724, Nighttime_photon_count: 0\n",
      "date: 2023-02-18, Daytime_photon_count: 0, Nighttime_photon_count: 81722\n",
      "date: 2023-03-05, Daytime_photon_count: 13103, Nighttime_photon_count: 0\n",
      "date: 2023-03-19, Daytime_photon_count: 0, Nighttime_photon_count: 143065\n",
      "date: 2023-04-17, Daytime_photon_count: 0, Nighttime_photon_count: 2072\n",
      "date: 2023-04-21, Daytime_photon_count: 0, Nighttime_photon_count: 69774\n",
      "date: 2023-05-06, Daytime_photon_count: 54436, Nighttime_photon_count: 0\n"
     ]
    }
   ],
   "source": [
    "def count_solar_elevation_values(processed_site_data):\n",
    "    \"\"\"\n",
    "    Counts how many entries have positive or 0 values and how many have negative values in the solar_elevation column.\n",
    "\n",
    "    Args:\n",
    "        processed_site_data (dict): A dictionary where each value is a DataFrame containing processed site data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the same keys as `processed_site_data`, containing the counts for positive/zero and negative values.\n",
    "    \"\"\"\n",
    "    elevation_counts = {}\n",
    "\n",
    "    for date, data in processed_site_data.items():\n",
    "        # Ensure the 'solar_elevation' column exists\n",
    "        if 'solar_elevation' in data.columns:\n",
    "            # Count positive or 0 values\n",
    "            positive_or_zero_count = (data['solar_elevation'] >= 0).sum()\n",
    "\n",
    "            # Count negative values\n",
    "            negative_count = (data['solar_elevation'] < 0).sum()\n",
    "\n",
    "            # Store counts in the dictionary\n",
    "            elevation_counts[date] = {\n",
    "                \"daytime\": positive_or_zero_count,\n",
    "                \"nighttime\": negative_count\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Warning: 'solar_elevation' column not found for {date}. Skipping.\")\n",
    "            elevation_counts[date] = {\"positive_or_zero\": 0, \"negative\": 0}\n",
    "\n",
    "    return elevation_counts\n",
    "\n",
    "# Example usage:\n",
    "solar_elev_counts = count_solar_elevation_values(processed_site_data)\n",
    "\n",
    "for date, counts in solar_elev_counts.items():\n",
    "    print(f\"date: {date}, Daytime_photon_count: {counts['daytime']}, Nighttime_photon_count: {counts['nighttime']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84824d40-eb13-4a23-8955-571485954240",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3929800175.py, line 87)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[58], line 87\u001b[0;36m\u001b[0m\n\u001b[0;31m    combined_slope, combined_intercept, *_ =\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def plot_all_sites_photon(processed_site_data, folder, site_name, solar_elev_counts, boundary_km):\n",
    "    # Initialize dictionaries for regression results and colors\n",
    "    regression_results = {'strong': {}, 'weak': {}}  # Separate results for strong and weak beam types\n",
    "    colors = {1: 'blue', 3: 'green', 5: 'purple', 2: 'orange', 4: 'brown', 6: 'pink'}\n",
    "\n",
    "    # Define landcover types\n",
    "    landcover_codes = {111: 'Closed forest, evergreen needle leaf', 115: 'Closed forest, mixed/unknown'}\n",
    "\n",
    "    for date, data in processed_site_data.items():\n",
    "        daytime_count = solar_elev_counts.get(date, {}).get(\"daytime\", 0)\n",
    "        nighttime_count = solar_elev_counts.get(date, {}).get(\"nighttime\", 0)\n",
    "        time_of_day = \"Daytime\" if daytime_count > nighttime_count else \"Nighttime\"\n",
    "        \n",
    "        # Loop over the two landcover codes\n",
    "        for landcover_code, landcover_label in landcover_codes.items():\n",
    "            # Filter the data based on the current landcover type\n",
    "            filtered_data = data[data['landcover_mode'] == landcover_code]\n",
    "            if filtered_data.empty:\n",
    "                continue  # Skip to the next landcover type if no data for this landcover\n",
    "\n",
    "            # Create a plot for this landcover type\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "            # Only loop through the weak beam types (2, 4, 6)\n",
    "            beam_type = 'weak'\n",
    "            beams = [2, 4, 6]\n",
    "    \n",
    "            beam_filtered_data = filtered_data.copy()\n",
    "            target_date = pd.to_datetime(date).date()\n",
    "            beam_filtered_data['constructed_date'] = pd.to_datetime(beam_filtered_data[['year', 'month', 'day']])\n",
    "    \n",
    "            # Filter by weak beams\n",
    "            beam_filter = beam_filtered_data['spot'].isin(beams)\n",
    "            filtered_data_for_beams = beam_filtered_data[(beam_filtered_data['constructed_date'].dt.date == target_date) & beam_filter]\n",
    "            filtered_data_for_beams = filtered_data_for_beams.dropna(subset=['canopy_photon_rate', 'terrain_photon_rate'])\n",
    "            filtered_data_for_beams = filtered_data_for_beams.drop_duplicates(subset=['canopy_photon_rate', 'terrain_photon_rate'])\n",
    "    \n",
    "            # Debugging statement for filtered data\n",
    "            print(f\"Filtered data for landcover {landcover_label}, date {date}, weak beams {beams}: {filtered_data_for_beams.shape[0]} points\")\n",
    "    \n",
    "            # Skip if no data for weak beams\n",
    "            if filtered_data_for_beams.empty:\n",
    "                print(f\"No data available for {target_date} with weak beams for landcover {landcover_label}.\")\n",
    "                plt.close()\n",
    "            else:\n",
    "                # Only plot scatter for beams with >= 10 points\n",
    "                valid_beams = [beam for beam in beams if len(filtered_data_for_beams[filtered_data_for_beams['spot'] == beam]) >= 10]\n",
    "    \n",
    "                if not valid_beams:\n",
    "                    print(f\"No valid data for {date} with weak beams for landcover {landcover_label}. Skipping plot.\")\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    # Initialize lists to store regression results and weights\n",
    "                    beam_regression_results = []\n",
    "                    weights = []\n",
    "    \n",
    "                    # Loop through each valid weak beam for plotting and regression\n",
    "                    for beam in valid_beams:\n",
    "                        beam_data = filtered_data_for_beams[filtered_data_for_beams['spot'] == beam]\n",
    "    \n",
    "                        # Debugging statement\n",
    "                        print(f\"Weak Beam {beam}: Number of Points: {len(beam_data)}\")\n",
    "    \n",
    "                        print(f\"Weak Beam {beam}: Color = {colors.get(beam, 'None')}\")\n",
    "                        ax.scatter(beam_data['terrain_photon_rate'], beam_data['canopy_photon_rate'], color=colors.get(beam, 'red'), label=f\"Beam {beam}\", alpha=0.6)\n",
    "    \n",
    "                        # Calculate weights based on terrain standard deviations\n",
    "                        terrain_mean = beam_data['terrain_photon_rate'].mean()\n",
    "                        terrain_std = beam_data['terrain_photon_rate'].std()\n",
    "                        terrain_deviation = np.abs((beam_data['terrain_photon_rate'] - terrain_mean) / terrain_std)\n",
    "                        terrain_weight = np.exp(-0.5 * (terrain_deviation ** 2))\n",
    "                        beam_weights = terrain_weight\n",
    "    \n",
    "                        # Perform weighted regression\n",
    "                        slope, intercept = weighted_regression(beam_data['terrain_photon_rate'], beam_data['canopy_photon_rate'], beam_weights)\n",
    "                        regression_line = slope * beam_data['terrain_photon_rate'] + intercept\n",
    "                        rv_rg = -1 * slope\n",
    "                        ax.plot(beam_data['terrain_photon_rate'], regression_line, linestyle='-', label=f\"Beam {beam} Rv/Rg: {rv_rg:.4f}\", color=colors.get(beam, 'red'))\n",
    "                        beam_regression_results.append(rv_rg)\n",
    "                        weights.append(len(beam_data))  # Store number of data points for each beam\n",
    "    \n",
    "                        # Store the regression results for weak beams\n",
    "                        regression_results[beam_type][landcover_code].setdefault(date, {})[beam] = {'slope': slope, 'intercept': intercept}\n",
    "    \n",
    "                    # Combined regression for the entire dataset of weak beams\n",
    "                    if len(beam_regression_results) > 1:\n",
    "                        combined_slope, combined_intercept, *_ = linregress(filtered_data_for_beams['terrain_photon_rate'], filtered_data_for_beams['canopy_photon_rate'])\n",
    "                        combined_line = combined_slope * filtered_data_for_beams['terrain_photon_rate'] + combined_intercept\n",
    "                        combined_rv_rg = -combined_slope\n",
    "                        ax.plot(filtered_data_for_beams['terrain_photon_rate'], combined_line, color='red', label=f\"Combined Rv/Rg: {combined_rv_rg:.4f}\")\n",
    "                        regression_results[beam_type][landcover_code].setdefault(date, {})['combined'] = {'rv_rg': combined_rv_rg, 'slope': combined_slope, 'intercept': combined_intercept}\n",
    "    \n",
    "                    # Calculate the weighted average Rv/Rg for weak beams with >= 10 data points\n",
    "                    if beam_regression_results:\n",
    "                        valid_slopes = [regression_results[beam_type][landcover_code].get(date, {}).get(beam, {}).get('slope') for beam in valid_beams]\n",
    "                        valid_intercepts = [regression_results[beam_type][landcover_code].get(date, {}).get(beam, {}).get('intercept') for beam in valid_beams]\n",
    "    \n",
    "                        # Calculate weighted average for intercepts and slopes\n",
    "                        weighted_avg_intercept = np.average(valid_intercepts, weights=weights)\n",
    "                        weighted_avg_slope = np.average(valid_slopes, weights=weights)\n",
    "                        weighted_avg_rv_rg = -weighted_avg_slope  # Since Rv/Rg = -slope\n",
    "    \n",
    "                        # Plot the weighted average regression line\n",
    "                        avg_regression_line = weighted_avg_slope * filtered_data_for_beams['terrain_photon_rate'] + weighted_avg_intercept\n",
    "                        ax.plot(filtered_data_for_beams['terrain_photon_rate'], avg_regression_line, color='black', linestyle=':', \n",
    "                                label=f\"Weighted Average Rv/Rg: {weighted_avg_rv_rg:.4f}\")\n",
    "    \n",
    "                        # Store the weighted average results\n",
    "                        regression_results[beam_type][landcover_code].setdefault(date, {})['weighted_average'] = {\n",
    "                            'rv_rg': weighted_avg_rv_rg, \n",
    "                            'slope': weighted_avg_slope, \n",
    "                            'intercept': weighted_avg_intercept\n",
    "                        }\n",
    "\n",
    "                # Plot settings\n",
    "                ax.set_xlabel('Terrain Photon Rate', fontsize=25)\n",
    "                ax.set_ylabel('Canopy Photon Rate', fontsize=25)\n",
    "                ax.set_title(f'{landcover_label}\\n{beam_type.capitalize()} Beams - {target_date} ({time_of_day}) {boundary_km}km', fontsize=30)\n",
    "                ax.legend(fontsize=15)\n",
    "                ax.grid(True)\n",
    "\n",
    "                # Save the plot\n",
    "                save_folder = os.path.join(folder, site_name, boundary_km)\n",
    "                os.makedirs(save_folder, exist_ok=True)\n",
    "                savefile = os.path.join(save_folder, f'{site_name}_{date}_{beam_type}_{landcover_code}_{boundary_km}km.png')\n",
    "                plt.savefig(savefile, bbox_inches='tight', dpi=300)\n",
    "                plt.show()\n",
    "                print(f\"Plot saved as {savefile}\")\n",
    "\n",
    "    return regression_results\n",
    "\n",
    "\n",
    "\n",
    "regression_results = plot_all_sites_photon(new_processed_site_data, day_folder, site_name, solar_elev_counts, boundary_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0af63f99-1e97-4195-a766-835f6a4e8109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data for landcover Closed forest, evergreen needle leaf, date 2022-09-05, weak beams [2, 4, 6]: 9 points\n",
      "No valid data for 2022-09-05 with weak beams for landcover Closed forest, evergreen needle leaf. Skipping plot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:149: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved as day/BONA/8/BONA_2022-09-05_weak_111_8km.png\n",
      "Filtered data for landcover Closed forest, mixed/unknown, date 2022-09-05, weak beams [2, 4, 6]: 0 points\n",
      "No data available for 2022-09-05 with weak beams for landcover Closed forest, mixed/unknown.\n",
      "Filtered data for landcover Closed forest, evergreen needle leaf, date 2022-09-18, weak beams [2, 4, 6]: 60 points\n",
      "Weak Beam 2: Number of Points: 25\n",
      "Weak Beam 2: Color = orange\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "111",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:269\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:113\u001b[0m, in \u001b[0;36mplot_all_sites_photon\u001b[0;34m(processed_site_data, folder, site_name, solar_elev_counts, boundary_km)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 111"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def weighted_regression(x, y, weights):\n",
    "    \"\"\"\n",
    "    Perform weighted linear regression.\n",
    "\n",
    "    Args:\n",
    "        x (array-like): Independent variable.\n",
    "        y (array-like): Dependent variable.\n",
    "        weights (array-like): Weights for the regression.\n",
    "\n",
    "    Returns:\n",
    "        slope, intercept: Coefficients of the weighted regression line.\n",
    "    \"\"\"\n",
    "    x = np.array(x).flatten()  # Convert x to a 1D array\n",
    "    y = np.array(y).flatten()  # Convert y to a 1D array\n",
    "    weights = np.array(weights).flatten()  # Flatten weights to ensure it's a 1D array\n",
    "\n",
    "    # Check if lengths match\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(f\"x and y must have the same length. Found x: {len(x)} and y: {len(y)}\")\n",
    "    if len(x) != len(weights):\n",
    "        raise ValueError(f\"x and weights must have the same length. Found x: {len(x)} and weights: {len(weights)}\")\n",
    "\n",
    "    x = sm.add_constant(x)  # Add intercept term\n",
    "    \n",
    "    weights = np.array(weights)  # Convert to NumPy array\n",
    "    model = sm.WLS(y, x, weights=weights)  # Weighted Least Squares\n",
    "    results = model.fit()\n",
    "    return results.params[1], results.params[0]  # slope, intercept\n",
    "\n",
    "def plot_all_sites_photon(processed_site_data, folder, site_name, solar_elev_counts, boundary_km):\n",
    "    regression_results = {'strong': {}, 'weak': {}}  # Separate results for strong and weak beam types\n",
    "    colors = {1: 'blue', 3: 'green', 5: 'purple', 2: 'orange', 4: 'brown', 6: 'pink'}\n",
    "\n",
    "    # Define landcover types\n",
    "    landcover_codes = {111: 'Closed forest, evergreen needle leaf', 115: 'Closed forest, mixed/unknown'}\n",
    "\n",
    "    for date, data in processed_site_data.items():\n",
    "        daytime_count = solar_elev_counts.get(date, {}).get(\"daytime\", 0)\n",
    "        nighttime_count = solar_elev_counts.get(date, {}).get(\"nighttime\", 0)\n",
    "        time_of_day = \"Daytime\" if daytime_count > nighttime_count else \"Nighttime\"\n",
    "        \n",
    "        # Loop over the two landcover codes\n",
    "        for landcover_code, landcover_label in landcover_codes.items():\n",
    "            # Filter the data based on the current landcover type\n",
    "            filtered_data = data[data['landcover_mode'] == landcover_code]\n",
    "            if filtered_data.empty:\n",
    "                continue  # Skip to the next landcover type if no data for this landcover\n",
    "\n",
    "            # Create a plot for this landcover type\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "\n",
    "            \n",
    "            # Only loop through the weak beam types (2, 4, 6)\n",
    "            beam_type = 'weak'\n",
    "            beams = [2, 4, 6]\n",
    "    \n",
    "            beam_filtered_data = filtered_data.copy()\n",
    "            target_date = pd.to_datetime(date).date()\n",
    "            beam_filtered_data['constructed_date'] = pd.to_datetime(beam_filtered_data[['year', 'month', 'day']])\n",
    "    \n",
    "            # Filter by weak beams\n",
    "            beam_filter = beam_filtered_data['spot'].isin(beams)\n",
    "            filtered_data_for_beams = beam_filtered_data[(beam_filtered_data['constructed_date'].dt.date == target_date) & beam_filter]\n",
    "            filtered_data_for_beams = filtered_data_for_beams.dropna(subset=['canopy_photon_rate', 'terrain_photon_rate'])\n",
    "            filtered_data_for_beams = filtered_data_for_beams.drop_duplicates(subset=['canopy_photon_rate', 'terrain_photon_rate'])\n",
    "    \n",
    "            # Debugging statement for filtered data\n",
    "            print(f\"Filtered data for landcover {landcover_label}, date {date}, weak beams {beams}: {filtered_data_for_beams.shape[0]} points\")\n",
    "    \n",
    "            # Skip if no data for weak beams\n",
    "            if filtered_data_for_beams.empty:\n",
    "                print(f\"No data available for {target_date} with weak beams for landcover {landcover_label}.\")\n",
    "                plt.close()\n",
    "            else:\n",
    "                # Only plot scatter for beams with >= 10 points\n",
    "                valid_beams = [beam for beam in beams if len(filtered_data_for_beams[filtered_data_for_beams['spot'] == beam]) >= 10]\n",
    "    \n",
    "                if not valid_beams:\n",
    "                    print(f\"No valid data for {date} with weak beams for landcover {landcover_label}. Skipping plot.\")\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    # Initialize lists to store regression results and weights\n",
    "                    beam_regression_results = []\n",
    "                    weights = []\n",
    "    \n",
    "                    # Loop through each valid weak beam for plotting and regression\n",
    "                    for beam in valid_beams:\n",
    "                        beam_data = filtered_data_for_beams[filtered_data_for_beams['spot'] == beam]\n",
    "    \n",
    "                        # Debugging statement\n",
    "                        print(f\"Weak Beam {beam}: Number of Points: {len(beam_data)}\")\n",
    "    \n",
    "                        print(f\"Weak Beam {beam}: Color = {colors.get(beam, 'None')}\")\n",
    "                        ax.scatter(beam_data['terrain_photon_rate'], beam_data['canopy_photon_rate'], color=colors[beam], label=f\"Beam {beam}\", alpha=0.6)\n",
    "    \n",
    "                        # Calculate weights based on terrain standard deviations\n",
    "                        terrain_mean = beam_data['terrain_photon_rate'].mean()\n",
    "                        terrain_std = beam_data['terrain_photon_rate'].std()\n",
    "                        terrain_deviation = np.abs((beam_data['terrain_photon_rate'] - terrain_mean) / terrain_std)\n",
    "                        terrain_weight = np.exp(-0.5 * (terrain_deviation ** 2))\n",
    "                        beam_weights = terrain_weight\n",
    "    \n",
    "                        # Perform weighted regression\n",
    "                        slope, intercept = weighted_regression(beam_data['terrain_photon_rate'], beam_data['canopy_photon_rate'], beam_weights)\n",
    "                        regression_line = slope * beam_data['terrain_photon_rate'] + intercept\n",
    "                        rv_rg = -1 * slope\n",
    "                        ax.plot(beam_data['terrain_photon_rate'], regression_line, linestyle='-', label=f\"Beam {beam} Rv/Rg: {rv_rg:.4f}\", color=colors[beam])\n",
    "                        beam_regression_results.append(rv_rg)\n",
    "                        weights.append(len(beam_data))  # Store number of data points for each beam\n",
    "    \n",
    "                        # Store the regression results for weak beams\n",
    "                        regression_results[beam_type][landcover_code].setdefault(date, {})[beam] = {'slope': slope, 'intercept': intercept}\n",
    "    \n",
    "                    # Combined regression for the entire dataset of weak beams\n",
    "                    if len(beam_regression_results) > 1:\n",
    "                        combined_slope, combined_intercept, *_ = linregress(filtered_data_for_beams['terrain_photon_rate'], filtered_data_for_beams['canopy_photon_rate'])\n",
    "                        combined_line = combined_slope * filtered_data_for_beams['terrain_photon_rate'] + combined_intercept\n",
    "                        combined_rv_rg = -combined_slope\n",
    "                        ax.plot(filtered_data_for_beams['terrain_photon_rate'], combined_line, color='red', label=f\"Combined Rv/Rg: {combined_rv_rg:.4f}\")\n",
    "                        regression_results[beam_type][landcover_code].setdefault(date, {})['combined'] = {'rv_rg': combined_rv_rg, 'slope': combined_slope, 'intercept': combined_intercept}\n",
    "    \n",
    "                    # Calculate the weighted average Rv/Rg for weak beams with >= 10 data points\n",
    "                    if beam_regression_results:\n",
    "                        valid_slopes = [regression_results[beam_type][landcover_code].get(date, {}).get(beam, {}).get('slope') for beam in valid_beams]\n",
    "                        valid_intercepts = [regression_results[beam_type][landcover_code].get(date, {}).get(beam, {}).get('intercept') for beam in valid_beams]\n",
    "    \n",
    "                        # Calculate weighted average for intercepts and slopes\n",
    "                        weighted_avg_intercept = np.average(valid_intercepts, weights=weights)\n",
    "                        weighted_avg_slope = np.average(valid_slopes, weights=weights)\n",
    "                        weighted_avg_rv_rg = -weighted_avg_slope  # Since Rv/Rg = -slope\n",
    "    \n",
    "                        # Plot the weighted average regression line\n",
    "                        avg_regression_line = weighted_avg_slope * filtered_data_for_beams['terrain_photon_rate'] + weighted_avg_intercept\n",
    "                        ax.plot(filtered_data_for_beams['terrain_photon_rate'], avg_regression_line, color='black', linestyle=':', \n",
    "                                label=f\"Weighted Average Rv/Rg: {weighted_avg_rv_rg:.4f}\")\n",
    "    \n",
    "                        # Store the weighted average results\n",
    "                        regression_results[beam_type][landcover_code].setdefault(date, {})['weighted_average'] = {\n",
    "                            'rv_rg': weighted_avg_rv_rg, \n",
    "                            'slope': weighted_avg_slope, \n",
    "                            'intercept': weighted_avg_intercept\n",
    "                        }\n",
    "\n",
    "                # Plot settings\n",
    "                ax.set_xlabel('Terrain Photon Rate', fontsize=25)\n",
    "                ax.set_ylabel('Canopy Photon Rate', fontsize=25)\n",
    "                ax.set_title(f'{landcover_label}\\n{beam_type.capitalize()} Beams - {target_date} ({time_of_day}) {boundary_km}km', fontsize=30)\n",
    "                ax.legend(fontsize=15)\n",
    "                ax.grid(True)\n",
    "\n",
    "                # Save the plot\n",
    "                save_folder = os.path.join(folder, site_name, boundary_km)\n",
    "                os.makedirs(save_folder, exist_ok=True)\n",
    "                savefile = os.path.join(save_folder, f'{site_name}_{date}_{beam_type}_{landcover_code}_{boundary_km}km.png')\n",
    "                plt.savefig(savefile, bbox_inches='tight', dpi=300)\n",
    "                plt.show()\n",
    "                print(f\"Plot saved as {savefile}\")\n",
    "\n",
    "            # # Loop through strong and weak beam types\n",
    "            # for beam_type, beams in {'strong': [1, 3, 5], 'weak': [2, 4, 6]}.items():\n",
    "            #     beam_filtered_data = filtered_data.copy()\n",
    "            #     regression_results[beam_type].setdefault(landcover_code, {}).setdefault(date, {})['time_of_day'] = time_of_day\n",
    "            #     valid_data_found = False\n",
    "            #     target_date = pd.to_datetime(date).date()\n",
    "            #     beam_filtered_data['constructed_date'] = pd.to_datetime(beam_filtered_data[['year', 'month', 'day']])\n",
    "            #     beam_filter = beam_filtered_data['spot'].isin(beams)\n",
    "            #     filtered_data_for_beams = beam_filtered_data[(beam_filtered_data['constructed_date'].dt.date == target_date) & beam_filter]\n",
    "            #     filtered_data_for_beams = filtered_data_for_beams.dropna(subset=['canopy_photon_rate', 'terrain_photon_rate'])\n",
    "            #     filtered_data_for_beams = filtered_data_for_beams.drop_duplicates(subset=['canopy_photon_rate', 'terrain_photon_rate'])\n",
    "                \n",
    "            #     # debug statement\n",
    "            #     print(f\"Filtered data for landcover {landcover_label}, date {date}, beam type {beam_type}{beams}: {filtered_data_for_beams.shape[0]} points\")\n",
    "\n",
    "            #     if filtered_data_for_beams.empty:\n",
    "            #         print(f\"No data available for {target_date} with {beam_type} beams for landcover {landcover_label}.\")\n",
    "            #         continue\n",
    "\n",
    "            #     # Only plot scatter for beams with >= 10 points\n",
    "            #     valid_beams = [beam for beam in beams if len(filtered_data_for_beams[filtered_data_for_beams['spot'] == beam]) >= 10]\n",
    "\n",
    "            #     # If no valid beams, skip to the next date\n",
    "            #     if not valid_beams:\n",
    "            #         print(f\"No valid data for {date} with {beam_type} beams for landcover {landcover_label}. Skipping plot.\")\n",
    "            #         plt.close()\n",
    "            #         continue\n",
    "\n",
    "            #     # Initialize lists to store regression results and weights\n",
    "            #     beam_regression_results = []\n",
    "            #     weights = []  # Store corresponding weights (data points per beam)\n",
    "\n",
    "            #     # Loop through each valid beam for plotting and regression\n",
    "            #     for beam in valid_beams:\n",
    "            #         beam_data = filtered_data_for_beams[filtered_data_for_beams['spot'] == beam]\n",
    "\n",
    "            #         # debug statement\n",
    "            #         beam_type_label = \"strong\" if beam in [1, 3, 5] else \"weak\"\n",
    "            #         print(f\"Beam Type: {beam_type_label.capitalize()}, Beam Number: {beam}, Number of Points: {len(beam_data)}\")\n",
    "\n",
    "            #         print(f\"Weak Beam {beam}: Color = {colors.get(beam, 'None')}\")\n",
    "            #         ax.scatter(beam_data['terrain_photon_rate'], beam_data['canopy_photon_rate'], color=colors[beam], label=f\"Beam {beam}\", alpha=0.6)\n",
    "\n",
    "            #         # Calculate weights based on terrain standard deviations\n",
    "            #         terrain_mean = beam_data['terrain_photon_rate'].mean()\n",
    "            #         terrain_std = beam_data['terrain_photon_rate'].std()\n",
    "            #         terrain_deviation = np.abs((beam_data['terrain_photon_rate'] - terrain_mean) / terrain_std)\n",
    "            #         terrain_weight = np.exp(-0.5 * (terrain_deviation ** 2))\n",
    "            #         beam_weights = terrain_weight\n",
    "\n",
    "            #         # Perform weighted regression\n",
    "            #         slope, intercept = weighted_regression(beam_data['terrain_photon_rate'], beam_data['canopy_photon_rate'], beam_weights)\n",
    "            #         regression_line = slope * beam_data['terrain_photon_rate'] + intercept\n",
    "            #         rv_rg = -1 * slope\n",
    "            #         ax.plot(beam_data['terrain_photon_rate'], regression_line, linestyle='-', label=f\"Beam {beam} Rv/Rg: {rv_rg:.4f}\", color=colors[beam])\n",
    "            #         beam_regression_results.append(rv_rg)\n",
    "            #         weights.append(len(beam_data))  # Store number of data points for each beam\n",
    "\n",
    "            #         regression_results[beam_type][landcover_code].setdefault(date, {})[beam] = {'slope': slope, 'intercept': intercept}\n",
    "\n",
    "            #     # Combined regression for the entire dataset\n",
    "            #     if len(beam_regression_results) > 1:\n",
    "            #         combined_slope, combined_intercept, *_ = linregress(filtered_data_for_beams['terrain_photon_rate'], filtered_data_for_beams['canopy_photon_rate'])\n",
    "            #         combined_line = combined_slope * filtered_data_for_beams['terrain_photon_rate'] + combined_intercept\n",
    "            #         combined_rv_rg = -combined_slope\n",
    "            #         ax.plot(filtered_data_for_beams['terrain_photon_rate'], combined_line, color='red', label=f\"Combined Rv/Rg: {combined_rv_rg:.4f}\")\n",
    "            #         regression_results[beam_type][landcover_code].setdefault(date, {})['combined'] = {'rv_rg': combined_rv_rg, 'slope': combined_slope, 'intercept': combined_intercept}\n",
    "\n",
    "\n",
    "            #     # Calculate the weighted average Rv/Rg for beams with >= 10 data points\n",
    "            #     if beam_regression_results:\n",
    "            #         valid_slopes = [regression_results[beam_type][landcover_code].get(date, {}).get(beam, {}).get('slope') for beam in valid_beams]\n",
    "            #         valid_intercepts = [regression_results[beam_type][landcover_code].get(date, {}).get(beam, {}).get('intercept') for beam in valid_beams]\n",
    "\n",
    "            #         # Calculate weighted average for intercepts and slopes\n",
    "            #         weighted_avg_intercept = np.average(valid_intercepts, weights=weights)\n",
    "            #         weighted_avg_slope = np.average(valid_slopes, weights=weights)\n",
    "            #         weighted_avg_rv_rg = -weighted_avg_slope  # Since Rv/Rg = -slope\n",
    "\n",
    "            #         # Plot the weighted average regression line\n",
    "            #         avg_regression_line = weighted_avg_slope * filtered_data_for_beams['terrain_photon_rate'] + weighted_avg_intercept\n",
    "            #         ax.plot(filtered_data_for_beams['terrain_photon_rate'], avg_regression_line, color='black', linestyle=':', \n",
    "            #                 label=f\"Weighted Average Rv/Rg: {weighted_avg_rv_rg:.4f}\")\n",
    "\n",
    "            #         # Store the weighted average results\n",
    "            #         regression_results[beam_type][landcover_code].setdefault(date, {})['weighted_average'] = {\n",
    "            #             'rv_rg': weighted_avg_rv_rg, \n",
    "            #             'slope': weighted_avg_slope, \n",
    "            #             'intercept': weighted_avg_intercept\n",
    "            #         }\n",
    "\n",
    "            #     # Plot settings\n",
    "            #     ax.set_xlabel('Terrain Photon Rate', fontsize=25)\n",
    "            #     ax.set_ylabel('Canopy Photon Rate', fontsize=25)\n",
    "            #     ax.set_title(f'{landcover_label}\\n{beam_type.capitalize()} Beams - {target_date} ({time_of_day}) {boundary_km}km', fontsize=30)\n",
    "            #     ax.legend(fontsize=15)\n",
    "            #     ax.grid(True)\n",
    "\n",
    "            #     save_folder = os.path.join(folder, site_name, boundary_km)\n",
    "            #     os.makedirs(save_folder, exist_ok=True)\n",
    "            #     savefile = os.path.join(save_folder, f'{site_name}_{date}_{beam_type}_{landcover_code}_{boundary_km}km.png')\n",
    "            #     plt.savefig(savefile, bbox_inches='tight', dpi=300)\n",
    "            #     plt.show()\n",
    "            #     print(f\"Plot saved as {savefile}\")\n",
    "\n",
    "            # plt.close()\n",
    "\n",
    "    return regression_results\n",
    "\n",
    "regression_results = plot_all_sites_photon(new_processed_site_data, day_folder, site_name, solar_elev_counts, boundary_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd7ff1b5-2a70-4fbe-8310-eee3196e05bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_landcover_codes_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 150\u001b[0m\n\u001b[1;32m    138\u001b[0m             plot_time_series(\n\u001b[1;32m    139\u001b[0m                 data\u001b[38;5;241m=\u001b[39mweighted_df,\n\u001b[1;32m    140\u001b[0m                 x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m                 color_map\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrong Weighted\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeak Weighted\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m    147\u001b[0m             )\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Call the modified function\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[43mplot_regression_time_series_by_landcover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregression_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msite_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mday_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary_km\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 96\u001b[0m, in \u001b[0;36mplot_regression_time_series_by_landcover\u001b[0;34m(regression_results, site_name, folder, boundary_km)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlot saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msavefile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Loop over landcover types and create plots\u001b[39;00m\n\u001b[1;32m     95\u001b[0m landcover_types \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 96\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClosed forest, evergreen needle leaf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_111\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mvalid_landcover_codes_1\u001b[49m),\n\u001b[1;32m     97\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClosed forest, mixed/unknown\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_115_116\u001b[39m\u001b[38;5;124m'\u001b[39m, valid_landcover_codes_2)\n\u001b[1;32m     98\u001b[0m ]\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m landcover_label, landcover_code, landcover_codes \u001b[38;5;129;01min\u001b[39;00m landcover_types:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Filter the regression results for the current landcover type\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     filtered_df_individual \u001b[38;5;241m=\u001b[39m df_individual[df_individual[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeam\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(landcover_codes)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_landcover_codes_1' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_regression_time_series_by_landcover(regression_results, site_name, folder, boundary_km):\n",
    "    \"\"\"\n",
    "    Plots time series of regression results for:\n",
    "    1. Individual beams with different colors\n",
    "    2. Strong and weak beams (combined regressions) with different colors\n",
    "    3. Weighted averages for strong and weak beams with different colors\n",
    "    For each landcover type (e.g., Closed forest, evergreen needle leaf and Closed forest, mixed/unknown),\n",
    "    the title and saved filename are adjusted accordingly.\n",
    "    \n",
    "    Args:\n",
    "        regression_results (dict): Regression results for strong and weak beams.\n",
    "        site_name (str): The name of the site for naming the plot.\n",
    "        folder (str): The folder where the plots should be saved.\n",
    "    \"\"\"\n",
    "    # Colors for individual beams\n",
    "    beam_colors = {1: 'blue', 3: 'green', 5: 'purple', 2: 'orange', 4: 'brown', 6: 'pink'}\n",
    "    time_series_data = {\n",
    "        'individual': [],\n",
    "        'strong_combined': [],\n",
    "        'weak_combined': [],\n",
    "        'strong_weighted': [],\n",
    "        'weak_weighted': []\n",
    "    }\n",
    "    \n",
    "    # Prepare data for the plots\n",
    "    for beam_type, date_results in regression_results.items():\n",
    "        for date, results in date_results.items():\n",
    "            # Parse date\n",
    "            date_obj = pd.to_datetime(date)\n",
    "            \n",
    "            # Individual beams\n",
    "            for beam, beam_result in results.items():\n",
    "                if beam == 'time_of_day':\n",
    "                    continue\n",
    "                if beam not in ['combined', 'weighted_average']:\n",
    "                    rv_rg = -beam_result['slope']\n",
    "                    time_series_data['individual'].append({'date': date_obj, 'beam': beam, 'rv_rg': rv_rg})\n",
    "            \n",
    "            # Combined regression\n",
    "            if 'combined' in results:\n",
    "                rv_rg = results['combined']['rv_rg']\n",
    "                time_series_data[f\"{beam_type}_combined\"].append({'date': date_obj, 'rv_rg': rv_rg})\n",
    "            \n",
    "            # Weighted average\n",
    "            if 'weighted_average' in results:\n",
    "                rv_rg = results['weighted_average']['rv_rg']\n",
    "                time_series_data[f\"{beam_type}_weighted\"].append({'date': date_obj, 'rv_rg': rv_rg})\n",
    "\n",
    "    # Convert to DataFrames for easier plotting\n",
    "    df_individual = pd.DataFrame(time_series_data['individual'])\n",
    "    df_strong_combined = pd.DataFrame(time_series_data['strong_combined'])\n",
    "    df_weak_combined = pd.DataFrame(time_series_data['weak_combined'])\n",
    "    df_strong_weighted = pd.DataFrame(time_series_data['strong_weighted'])\n",
    "    df_weak_weighted = pd.DataFrame(time_series_data['weak_weighted'])\n",
    "\n",
    "    def plot_time_series(data, x_col, y_col, hue_col, title, y_label, save_name, color_map=None):\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        # If color_map is not provided, use default colors for 'Strong' and 'Weak' categories\n",
    "        if not color_map:\n",
    "            color_map = {'Strong': 'orange', 'Weak': 'blue'}  # Example color map for Strong and Weak beams\n",
    "            \n",
    "        for key, group in data.groupby(hue_col):\n",
    "            color = color_map.get(key, None) if color_map else None\n",
    "            ax.plot(group[x_col], group[y_col], label=key, color=color, marker='o')\n",
    "        \n",
    "        # Title and labels\n",
    "        ax.set_title(title, fontsize=25)\n",
    "        ax.set_xlabel('Date', fontsize=25)\n",
    "        ax.set_ylabel(y_label, fontsize=25)\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=14)\n",
    "        \n",
    "        # Add legend\n",
    "        ax.legend(title=hue_col.capitalize(), fontsize=15)\n",
    "        \n",
    "        # Add grid\n",
    "        ax.grid(True, which='both', linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Save the plot\n",
    "        savefolder = os.path.join(folder, site_name, boundary_km)\n",
    "\n",
    "        # Create the directory structure if it doesn't exist\n",
    "        if not os.path.exists(savefolder):\n",
    "            os.makedirs(savefolder)\n",
    "            print(f\"Created directory: {savefolder}\")\n",
    "        \n",
    "        savefile = os.path.join(savefolder, f\"{site_name}_{save_name}_{boundary_km}km.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(savefile, dpi=300)\n",
    "        plt.show()\n",
    "        print(f\"Plot saved: {savefile}\")\n",
    "    \n",
    "    # Loop over landcover types and create plots\n",
    "    landcover_types = [\n",
    "        ('Closed forest, evergreen needle leaf', '_111', valid_landcover_codes_1),\n",
    "        ('Closed forest, mixed/unknown', '_115_116', valid_landcover_codes_2)\n",
    "    ]\n",
    "    \n",
    "    for landcover_label, landcover_code, landcover_codes in landcover_types:\n",
    "        # Filter the regression results for the current landcover type\n",
    "        filtered_df_individual = df_individual[df_individual['beam'].isin(landcover_codes)]\n",
    "        filtered_df_strong_combined = df_strong_combined[df_strong_combined['beam'].isin(landcover_codes)]\n",
    "        filtered_df_weak_combined = df_weak_combined[df_weak_combined['beam'].isin(landcover_codes)]\n",
    "        filtered_df_strong_weighted = df_strong_weighted[df_strong_weighted['beam'].isin(landcover_codes)]\n",
    "        filtered_df_weak_weighted = df_weak_weighted[df_weak_weighted['beam'].isin(landcover_codes)]\n",
    "\n",
    "        # Plot 1: Individual beams\n",
    "        if not filtered_df_individual.empty:\n",
    "            plot_time_series(\n",
    "                data=filtered_df_individual,\n",
    "                x_col='date',\n",
    "                y_col='rv_rg',\n",
    "                hue_col='beam',\n",
    "                title=f\"Rv/Rg Time Series for Individual Beams at {site_name} {landcover_label} {boundary_km}km\",\n",
    "                y_label='Rv/Rg',\n",
    "                save_name=f'individual_beams_time_series{landcover_code}'\n",
    "            )\n",
    "\n",
    "        # Plot 2: Strong and weak combined regressions\n",
    "        if not filtered_df_strong_combined.empty or not filtered_df_weak_combined.empty:\n",
    "            combined_df = pd.concat([filtered_df_strong_combined.assign(beam_type='Strong'),\n",
    "                                     filtered_df_weak_combined.assign(beam_type='Weak')])  \n",
    "            plot_time_series(\n",
    "                data=combined_df,\n",
    "                x_col='date',\n",
    "                y_col='rv_rg',\n",
    "                hue_col='beam_type',\n",
    "                title=f\"Combined Rv/Rg Time Series for \\nStrong and Weak Beams at {site_name} {landcover_label} {boundary_km}km\",\n",
    "                y_label='Rv/Rg',\n",
    "                save_name=f'combined_beams_time_series{landcover_code}'\n",
    "            )\n",
    "            \n",
    "        # Plot 3: Weighted averages for strong and weak beams\n",
    "        if not filtered_df_strong_weighted.empty or not filtered_df_weak_weighted.empty:\n",
    "            weighted_df = pd.concat([filtered_df_strong_weighted.assign(beam_type='Strong Weighted'),\n",
    "                                     filtered_df_weak_weighted.assign(beam_type='Weak Weighted')])  \n",
    "            plot_time_series(\n",
    "                data=weighted_df,\n",
    "                x_col='date',\n",
    "                y_col='rv_rg',\n",
    "                hue_col='beam_type',\n",
    "                title=f\"Weighted Average Rv/Rg Time Series for \\nStrong and Weak Beams at {site_name} {landcover_label} {boundary_km}km\",\n",
    "                y_label='Rv/Rg',\n",
    "                save_name=f'weighted_avg_beams_time_series{landcover_code}',\n",
    "                color_map={'Strong Weighted': 'orange', 'Weak Weighted': 'blue'}\n",
    "            )\n",
    "\n",
    "# Call the modified function\n",
    "plot_regression_time_series_by_landcover(regression_results, site_name, day_folder, boundary_km)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a43acf3c-46f5-41bc-b2f1-230c24b854bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-05\n",
      "{'time_of_day': 'Nighttime', 5: {'slope': -0.7083977010394046, 'intercept': 1.1854160512183634}, 'weighted_average': {'rv_rg': 0.7083977010394046, 'slope': -0.7083977010394046, 'intercept': 1.1854160512183634}}\n",
      "2022-09-18\n",
      "{'time_of_day': 'Daytime', 1: {'slope': -0.6402591913802163, 'intercept': 1.405523268781069}, 3: {'slope': -0.8255667882612518, 'intercept': 1.3445810157155946}, 'combined': {'rv_rg': 0.15677507814420075, 'slope': -0.15677507814420075, 'intercept': 0.8352698430892733}, 'weighted_average': {'rv_rg': 0.7354262031599886, 'slope': -0.7354262031599886, 'intercept': 1.3742256198288183}}\n",
      "2022-10-17\n",
      "{'time_of_day': 'Nighttime', 5: {'slope': -0.41443093699450884, 'intercept': 1.0713939574143845}, 'weighted_average': {'rv_rg': 0.41443093699450884, 'slope': -0.41443093699450884, 'intercept': 1.0713939574143845}}\n",
      "2022-11-06\n",
      "{'time_of_day': 'Nighttime', 1: {'slope': -0.5108882282823202, 'intercept': 3.588318143765002}, 3: {'slope': -0.4943859250722393, 'intercept': 2.7210567199715827}, 5: {'slope': -0.37312720472999417, 'intercept': 3.094712936760461}, 'combined': {'rv_rg': 0.36385100450923874, 'slope': -0.36385100450923874, 'intercept': 2.765365569759413}, 'weighted_average': {'rv_rg': 0.4580236841536353, 'slope': -0.4580236841536353, 'intercept': 3.1258739241078524}}\n",
      "2022-11-19\n",
      "{'time_of_day': 'Nighttime'}\n",
      "2022-12-18\n",
      "{'time_of_day': 'Nighttime', 3: {'slope': -0.7533776996285914, 'intercept': 4.034982075990539}, 5: {'slope': -0.7486108365773406, 'intercept': 4.858353450050969}, 'combined': {'rv_rg': 0.6348919801349826, 'slope': -0.6348919801349826, 'intercept': 4.104107373274036}, 'weighted_average': {'rv_rg': 0.7508776872131256, 'slope': -0.7508776872131256, 'intercept': 4.46680456292984}}\n",
      "2023-01-20\n",
      "{'time_of_day': 'Nighttime', 5: {'slope': -0.049973042221171354, 'intercept': 0.7441055932944237}, 'weighted_average': {'rv_rg': 0.049973042221171354, 'slope': -0.049973042221171354, 'intercept': 0.7441055932944237}}\n",
      "2023-01-31\n",
      "{'time_of_day': 'Daytime', 1: {'slope': -0.3283575057398364, 'intercept': 1.2460582221507648}, 3: {'slope': -0.948134380447731, 'intercept': 1.5468117711691818}, 5: {'slope': -1.0184594677487953, 'intercept': 1.347814014857919}, 'combined': {'rv_rg': 0.2800498021799639, 'slope': -0.2800498021799639, 'intercept': 1.0766057720858613}, 'weighted_average': {'rv_rg': 0.5778004502818883, 'slope': -0.5778004502818883, 'intercept': 1.3459100852740755}}\n",
      "2023-02-18\n",
      "{'time_of_day': 'Nighttime', 1: {'slope': -0.5720140043553772, 'intercept': 3.96400831255035}, 'weighted_average': {'rv_rg': 0.5720140043553772, 'slope': -0.5720140043553772, 'intercept': 3.96400831255035}}\n",
      "2023-03-05\n",
      "{'time_of_day': 'Daytime', 1: {'slope': -0.2900556260344649, 'intercept': 0.9154826963277765}, 3: {'slope': -0.6692922046605065, 'intercept': 1.6184511669671902}, 'combined': {'rv_rg': 0.3494027310446607, 'slope': -0.3494027310446607, 'intercept': 1.1625482306123915}, 'weighted_average': {'rv_rg': 0.4172678960799093, 'slope': -0.4172678960799093, 'intercept': 1.1512885757194786}}\n",
      "2023-03-19\n",
      "{'time_of_day': 'Nighttime', 1: {'slope': -0.5906988453467998, 'intercept': 3.7907071999087507}, 3: {'slope': -0.7663817606563248, 'intercept': 3.074187256753766}, 'combined': {'rv_rg': 0.4021528197783098, 'slope': -0.4021528197783098, 'intercept': 2.5291011488454247}, 'weighted_average': {'rv_rg': 0.6839750982233678, 'slope': -0.6839750982233678, 'intercept': 3.4102815034710683}}\n",
      "2023-04-17\n",
      "{'time_of_day': 'Nighttime', 1: {'slope': -1.4789665043243199, 'intercept': 1.8798927174463063}, 'weighted_average': {'rv_rg': 1.4789665043243199, 'slope': -1.4789665043243199, 'intercept': 1.8798927174463063}}\n",
      "2023-04-21\n",
      "{'time_of_day': 'Nighttime', 1: {'slope': -0.5250631517937502, 'intercept': 2.90582655511621}, 'weighted_average': {'rv_rg': 0.5250631517937502, 'slope': -0.5250631517937502, 'intercept': 2.90582655511621}}\n",
      "2023-05-06\n",
      "{'time_of_day': 'Daytime', 5: {'slope': -0.40147155985407756, 'intercept': 2.1488437994638447}, 'weighted_average': {'rv_rg': 0.40147155985407756, 'slope': -0.40147155985407756, 'intercept': 2.1488437994638447}}\n",
      "2022-09-05\n",
      "{'time_of_day': 'Nighttime', 6: {'slope': -0.6123638999544191, 'intercept': 0.7927326933115421}, 'weighted_average': {'rv_rg': 0.6123638999544191, 'slope': -0.6123638999544191, 'intercept': 0.7927326933115421}}\n",
      "2022-09-18\n",
      "{'time_of_day': 'Daytime', 2: {'slope': -0.7220317211822748, 'intercept': 1.0814356872409583}, 4: {'slope': -0.6663905315717681, 'intercept': 0.9560092306419649}, 'combined': {'rv_rg': 0.3300759798715439, 'slope': -0.3300759798715439, 'intercept': 0.7758461294971044}, 'weighted_average': {'rv_rg': 0.6959944978388967, 'slope': -0.6959944978388967, 'intercept': 1.0227425376786088}}\n",
      "2022-10-17\n",
      "{'time_of_day': 'Nighttime', 6: {'slope': -0.6906548555305435, 'intercept': 0.9702797293630803}, 'weighted_average': {'rv_rg': 0.6906548555305435, 'slope': -0.6906548555305435, 'intercept': 0.9702797293630803}}\n",
      "2022-11-06\n",
      "{'time_of_day': 'Nighttime', 2: {'slope': -0.635826971271323, 'intercept': 1.3240592340419637}, 4: {'slope': -0.5856158912668145, 'intercept': 1.1831710003985703}, 6: {'slope': -0.6229792374677726, 'intercept': 1.421094214303361}, 'combined': {'rv_rg': 0.6174421297345326, 'slope': -0.6174421297345326, 'intercept': 1.3457260774965119}, 'weighted_average': {'rv_rg': 0.6137850708440087, 'slope': -0.6137850708440087, 'intercept': 1.309898897754516}}\n",
      "2022-11-19\n",
      "{'time_of_day': 'Nighttime'}\n",
      "2022-12-18\n",
      "{'time_of_day': 'Nighttime', 4: {'slope': -0.8987779634813791, 'intercept': 1.839375386185698}, 6: {'slope': -0.8232141055222288, 'intercept': 1.7685324889495972}, 'combined': {'rv_rg': 0.9070364442758695, 'slope': -0.9070364442758695, 'intercept': 1.9140681200370016}, 'weighted_average': {'rv_rg': 0.8603599750913733, 'slope': -0.8603599750913733, 'intercept': 1.8033576168838419}}\n",
      "2023-01-20\n",
      "{'time_of_day': 'Nighttime', 6: {'slope': -0.6478282678448615, 'intercept': 1.073216094716324}, 'weighted_average': {'rv_rg': 0.6478282678448616, 'slope': -0.6478282678448616, 'intercept': 1.073216094716324}}\n",
      "2023-01-31\n",
      "{'time_of_day': 'Daytime', 2: {'slope': -0.834668609938292, 'intercept': 1.250372142288928}, 4: {'slope': -0.8753894549774542, 'intercept': 1.1724624228515896}, 'combined': {'rv_rg': 0.8235853308428093, 'slope': -0.8235853308428093, 'intercept': 1.2361010331199864}, 'weighted_average': {'rv_rg': 0.8449112764818849, 'slope': -0.8449112764818849, 'intercept': 1.2307752189948735}}\n",
      "2023-02-18\n",
      "{'time_of_day': 'Nighttime', 2: {'slope': -0.762172476713695, 'intercept': 1.5427244898331758}, 'weighted_average': {'rv_rg': 0.762172476713695, 'slope': -0.762172476713695, 'intercept': 1.5427244898331758}}\n",
      "2023-03-05\n",
      "{'time_of_day': 'Daytime', 2: {'slope': -0.6115398032826305, 'intercept': 1.0295829398334875}, 4: {'slope': -0.7375632694282422, 'intercept': 1.3367565832212687}, 'combined': {'rv_rg': 0.7659378393617575, 'slope': -0.7659378393617575, 'intercept': 1.2971862350979968}, 'weighted_average': {'rv_rg': 0.6721280081603285, 'slope': -0.6721280081603285, 'intercept': 1.1772625760776132}}\n",
      "2023-03-19\n",
      "{'time_of_day': 'Nighttime', 2: {'slope': -0.849945863615965, 'intercept': 1.617050628728559}, 4: {'slope': -0.9822340644554511, 'intercept': 1.7072604738977428}, 'combined': {'rv_rg': 0.9065371005335825, 'slope': -0.9065371005335825, 'intercept': 1.7022276462263914}, 'weighted_average': {'rv_rg': 0.9163386260673613, 'slope': -0.9163386260673613, 'intercept': 1.6623251186912884}}\n",
      "2023-04-17\n",
      "{'time_of_day': 'Nighttime'}\n",
      "2023-04-21\n",
      "{'time_of_day': 'Nighttime', 2: {'slope': -0.9001734542054287, 'intercept': 1.620018284015363}, 'weighted_average': {'rv_rg': 0.9001734542054286, 'slope': -0.9001734542054286, 'intercept': 1.620018284015363}}\n",
      "2023-05-06\n",
      "{'time_of_day': 'Daytime', 6: {'slope': -0.8521117503164244, 'intercept': 1.5686492138636314}, 'weighted_average': {'rv_rg': 0.8521117503164244, 'slope': -0.8521117503164244, 'intercept': 1.5686492138636314}}\n"
     ]
    }
   ],
   "source": [
    "for beam_type, date_results in regression_results.items():\n",
    "    for date, results in date_results.items():\n",
    "        print(date)\n",
    "        # print(results.get(\"time_of_day\") == \"Nighttime\")\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "168cb75d-3258-4e0c-bccf-f57e72644d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strong Beams Regression Results After 2023-07-01:\n",
      "\n",
      "Weak Beams Regression Results After 2023-07-01:\n"
     ]
    }
   ],
   "source": [
    "def output_regression_results_after_date(regression_results, date_cutoff='2023-07-01'):\n",
    "    \"\"\"\n",
    "    Outputs the regression results for each beam, combined regression, and weighted regression\n",
    "    after a specified date.\n",
    "\n",
    "    Args:\n",
    "        regression_results (dict): The regression results for each beam and the combined/weighted results.\n",
    "        date_cutoff (str): The cutoff date in 'YYYY-MM-DD' format. Default is '2023-07-01'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Convert the cutoff date to a datetime object\n",
    "    date_cutoff = pd.to_datetime(date_cutoff)\n",
    "\n",
    "    # Loop through the regression results and output results after the cutoff date\n",
    "    for beam_type in ['strong', 'weak']:\n",
    "        print(f\"\\n{beam_type.capitalize()} Beams Regression Results After {date_cutoff.date()}:\")\n",
    "\n",
    "        for date, results in regression_results[beam_type].items():\n",
    "            # # Check if the time_of_day is nighttime\n",
    "            if results.get(\"time_of_day\") != \"Nighttime\":\n",
    "                continue\n",
    "            # Filter by date after the cutoff\n",
    "            if pd.to_datetime(date) >= date_cutoff:\n",
    "                print(f\"\\nResults for {date}:\")\n",
    "                \n",
    "                # For each beam, output its regression results (slope, intercept, Rv/Rg)\n",
    "                for beam, result in results.items():\n",
    "                    if beam != 'combined' and beam != 'weighted_average':  # Exclude combined and weighted average from individual beams\n",
    "                        print(f\"  Beam {beam}:\")\n",
    "                        print(f\"    Slope: {result['slope']}\")\n",
    "                        print(f\"    Intercept: {result['intercept']}\")\n",
    "                        print(f\"    Rv/Rg: {-result['slope']}\")\n",
    "                \n",
    "                # Output combined regression results\n",
    "                if 'combined' in results:\n",
    "                    print(f\"  Combined Regression:\")\n",
    "                    print(f\"    Slope: {results['combined']['slope']}\")\n",
    "                    print(f\"    Intercept: {results['combined']['intercept']}\")\n",
    "                    print(f\"    Rv/Rg: {-results['combined']['slope']}\")\n",
    "                \n",
    "                # Output weighted average regression results\n",
    "                if 'weighted_average' in results:\n",
    "                    print(f\"  Weighted Average Regression:\")\n",
    "                    print(f\"    Slope: {results['weighted_average']['slope']}\")\n",
    "                    print(f\"    Intercept: {results['weighted_average']['intercept']}\")\n",
    "                    print(f\"    Rv/Rg: {-results['weighted_average']['slope']}\")\n",
    "\n",
    "# Example usage\n",
    "output_regression_results_after_date(regression_results, '2023-07-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda1f076-8da9-4587-852f-df46ad8edc76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
