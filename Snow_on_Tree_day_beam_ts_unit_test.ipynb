{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ba60c5-79d0-4678-b0cb-6e3d771ddc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simplekml in /srv/conda/envs/notebook/lib/python3.11/site-packages (1.3.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: geopy in /srv/conda/envs/notebook/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from geopy) (2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: kaleido in /srv/conda/envs/notebook/lib/python3.11/site-packages (0.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install simplekml\n",
    "%pip install geopy\n",
    "%pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed7a6ce-b20e-48d6-8c9c-b5cfc2034128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58 µs, sys: 0 ns, total: 58 µs\n",
      "Wall time: 60.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sliderule import sliderule, icesat2, earthdata\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import folium\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, Point, mapping\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import geopy\n",
    "import simplekml\n",
    "from geopy.distance import geodesic\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from scipy.stats import linregress\n",
    "import statsmodels.api as sm\n",
    "import dask.array as da\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93bb138-e02b-476a-a29d-db3eeccb662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "# Start the Dask client, this will use your local machine's resources\n",
    "client = Client()\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7fd59b2-f1da-4fc1-9d48-c66b4a8cd760",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_meter = \"20\"\n",
    "cnt = \"10\"\n",
    "ats = \"10\"\n",
    "\n",
    "site_name = \"BONA\"\n",
    "# site_name = \"DEJU\"\n",
    "# site_name = \"WREF\"\n",
    "# site_name = \"RMNP\"\n",
    "# site_name = \"TEAK\"\n",
    "\n",
    "boundary_km = \"8\"\n",
    "\n",
    "year_folder = 'year/'\n",
    "day_folder = 'day/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83afef08-3860-4c2e-a828-70a7761ab06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BONA\n",
      "['2019-01-24', '2019-02-12', '2019-02-26', '2019-03-13', '2019-04-25', '2019-05-28', '2019-06-12', '2019-06-26', '2019-07-15', '2019-08-26', '2019-09-11', '2019-09-24', '2019-11-12', '2019-12-11', '2019-12-24', '2020-01-13', '2020-01-22', '2020-02-11', '2020-02-24', '2020-03-11', '2020-04-22', '2020-05-11', '2020-07-12', '2020-08-10', '2020-09-22', '2020-11-09', '2020-11-23', '2020-12-08', '2021-02-08', '2021-02-21', '2021-03-09', '2021-03-22', '2021-06-21', '2021-07-20', '2021-09-06', '2021-10-19', '2021-11-07', '2022-02-20', '2022-03-07', '2022-03-21', '2022-04-19', '2022-05-21', '2022-06-06', '2022-06-19', '2022-08-07', '2022-08-20', '2022-09-05', '2022-09-18', '2022-10-08', '2022-10-17', '2022-11-06', '2022-11-19', '2022-12-05', '2022-12-18', '2023-01-16', '2023-01-20', '2023-01-31', '2023-02-04', '2023-02-18', '2023-03-05', '2023-03-19', '2023-04-17', '2023-04-21', '2023-05-06', '2023-05-20', '2023-07-17', '2023-08-05', '2023-09-16', '2023-10-15', '2023-11-04', '2023-11-13']\n"
     ]
    }
   ],
   "source": [
    "def compile_dates_for_site(folder, site_name):\n",
    "    \"\"\"\n",
    "    Reads all CSV files for the specified site in a folder, extracts the 'date' column,\n",
    "    and compiles a unique list of dates.\n",
    "\n",
    "    Args:\n",
    "        folder (str): The directory containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of unique dates for the specified site.\n",
    "    \"\"\"\n",
    "    print(site_name)\n",
    "    all_dates = set()  # Use a set to ensure unique dates\n",
    "\n",
    "    # Iterate through all files in the folder\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.startswith(site_name) and filename.endswith(\".csv\"):  # Match site_name in the file name\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            try:\n",
    "                # Read the CSV file into a DataFrame\n",
    "                df = pd.read_csv(filepath)\n",
    "                if 'date' in df.columns and 'ground_photon_count' in df.columns and 'canopy_photon_count' in df.columns:\n",
    "                    # Filter rows where both ground_photon_count and canopy_photon_count are 0\n",
    "                    filtered_df = df[(df['ground_photon_count'] != 0) | (df['canopy_photon_count'] != 0)]\n",
    "                    # Add unique dates directly from the 'date' column as strings\n",
    "                    all_dates.update(filtered_df['date'].dropna().unique())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    # Return sorted list of unique dates\n",
    "    return sorted(all_dates)\n",
    "\n",
    "site_dates = compile_dates_for_site(year_folder, site_name)\n",
    "print(site_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35c7c309-0646-49a5-abff-2544273159c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BONA\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "def generate_date_time_ranges(dates):\n",
    "    \"\"\"\n",
    "    Generate a dictionary of start and end times for each unique date.\n",
    "\n",
    "    Args:\n",
    "        dates (list of str): List of date strings in the format \"YYYY-MM-DD\".\n",
    "        \n",
    "    Returns:\n",
    "        list of dict: A list of dictionaries where each dictionary represents a time range\n",
    "                      with 'start_time' and 'end_time' for a specific date.\n",
    "    \"\"\"\n",
    "    time_ranges = []\n",
    "    for date in dates:\n",
    "        start_time = f\"{date}T00:00:00Z\"\n",
    "        end_time = f\"{date}T23:59:59Z\"\n",
    "        \n",
    "        # Append the date with its start_time and end_time to the time_ranges list\n",
    "        time_ranges.append({\n",
    "            \"date\": date, \n",
    "            \"start_time\": start_time, \n",
    "            \"end_time\": end_time\n",
    "        })\n",
    "    return time_ranges\n",
    "\n",
    "site_time_ranges = generate_date_time_ranges(site_dates)\n",
    "print(site_name)\n",
    "# for date in site_time_ranges:\n",
    "#     print(date)\n",
    "print(len(site_time_ranges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea4d38-d0a8-46cb-89cc-b7a850f61d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "@delayed\n",
    "def process_time_range(time_range, site, boundary_km, cnt, ats, segment_meter):\n",
    "    \"\"\"\n",
    "    Processes ATL03 data for a specific time range.\n",
    "    \"\"\"\n",
    "    # Load the region dynamically based on the site\n",
    "    region = sliderule.toregion(f\"geojson_files/{site}_buffer_{boundary_km}km.geojson\")\n",
    "    \n",
    "    # Construct the parameters for the current time range\n",
    "    parms = {\n",
    "        \"poly\": region['poly'],       # Region polygon\n",
    "        \"t0\": time_range['start_time'],   # Start time\n",
    "        \"t1\": time_range['end_time'],     # End time\n",
    "        \"srt\": icesat2.SRT_LAND,           # Surface return type\n",
    "        \"cnf\": 0,                    # Confidence level\n",
    "        \"cnt\": cnt,                  # Number of photons\n",
    "        \"ats\": ats,                  # Along-track spacing\n",
    "        \"len\": segment_meter,        # Segment length\n",
    "        \"res\": segment_meter,        # Resolution\n",
    "        \"atl08_class\": [             # ATL08 classifications\n",
    "            \"atl08_noise\",\n",
    "            \"atl08_ground\",\n",
    "            \"atl08_canopy\",\n",
    "            \"atl08_top_of_canopy\",\n",
    "            \"atl08_unclassified\"\n",
    "        ],\n",
    "        \"phoreal\": {                 # Phoreal processing settings\n",
    "            \"binsize\": 1.0,\n",
    "            \"geoloc\": \"mean\",\n",
    "            \"use_abs_h\": True,\n",
    "            \"send_waveform\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Request the ATL03 data using icesat2\n",
    "    atl03_data = icesat2.atl03sp(parms, keep_id=True)\n",
    "    \n",
    "    # If only the 'geometry' column remains, skip this date\n",
    "    if atl03_data.shape[1] == 1 and 'geometry' in atl03_data.columns:\n",
    "        print(f\"Skipping {time_range['date']} because it only contains geometry data.\")\n",
    "        return None  # Return None for skipped data\n",
    "    \n",
    "    columns_to_drop = [\n",
    "        'region', 'pair', 'segment_dist', 'segment_id', 'cycle', 'track', 'background_rate', 'y_atc', \n",
    "        'yapc_score', 'atl03_cnf', 'relief', 'quality_ph'\n",
    "    ]\n",
    "    \n",
    "    # Drop the columns from the DataFrame\n",
    "    atl03_data = atl03_data.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Return the processed data along with the date key\n",
    "    return time_range['date'], atl03_data\n",
    "\n",
    "def process_site(site, time_ranges, boundary_km, cnt, ats, segment_meter):\n",
    "    \"\"\"\n",
    "    Processes a site by requesting ATL03 data for specified time ranges and returns a dictionary of processed data.\n",
    "    This function now uses Dask for parallel processing.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store processed data\n",
    "    site_data = {}\n",
    "\n",
    "    # Create a list of delayed tasks for each time range\n",
    "    tasks = []\n",
    "    for time_range in time_ranges:\n",
    "        task = process_time_range(time_range, site, boundary_km, cnt, ats, segment_meter)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    # Compute the results in parallel using Dask\n",
    "    results = client.compute(tasks, sync=True)  # The 'sync=True' option will block and collect results\n",
    "\n",
    "    # Store the processed data in the dictionary with the date as the key\n",
    "    for date, atl03_data in results:\n",
    "        if atl03_data is not None:  # Skip any None values returned for skipped dates\n",
    "            site_data[date] = atl03_data\n",
    "\n",
    "    return site_data\n",
    "\n",
    "site_data = process_site(site_name, site_time_ranges, boundary_km, cnt, ats, segment_meter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04e08f2-9730-499f-9631-ca0363ab4eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.4 ms, sys: 4.48 ms, total: 32.9 ms\n",
      "Wall time: 36.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# For testing the new moving window functionality\n",
    "\n",
    "segment_meter = \"20\"\n",
    "cnt = \"10\"\n",
    "ats = \"10\"\n",
    "start_time = \"2022-10-01T00:00:00Z\"\n",
    "end_time =   \"2023-05-31T23:59:59Z\"\n",
    "\n",
    "site_name = \"BONA\"\n",
    "boundary_km = \"8\"\n",
    "\n",
    "region = sliderule.toregion(f\"geojson_files/{site_name}_buffer_{boundary_km}km.geojson\")\n",
    "\n",
    "parms = {\n",
    "    \"poly\": region['poly'],# Polygon defining the spatial region of interest\n",
    "    \"t0\": start_time, # Start time of the data collection period (ISO 8601 format)\n",
    "    \"t1\": end_time, # End time of the data collection period (ISO 8601 format)\n",
    "    \"srt\": icesat2.SRT_LAND, # Surface return type: specifies the data type (e.g., LAND, ICE, etc.)\n",
    "    \"cnf\": 0, # Confidence level for photon classification (e.g., 0: all photons)\n",
    "    \"cnt\": cnt, # Number of photons required for the analysis (optional)\n",
    "    \"ats\": ats, # Along-track spacing for the data (optional, controls density of data)\n",
    "    \"len\": segment_meter,# Segment length in meters, used for segmentation \n",
    "    \"res\": segment_meter, # Resolution of the output, in meters\n",
    "    # List of ATL08 photon classifications to include in the data\n",
    "    \"atl08_class\": [\n",
    "        \"atl08_noise\",        # Photons classified as noise\n",
    "        \"atl08_ground\",       # Ground-level photons\n",
    "        \"atl08_canopy\",       # Photons classified as canopy\n",
    "        \"atl08_top_of_canopy\",# Photons classified as the top of the canopy\n",
    "        \"atl08_unclassified\"  # Unclassified photons\n",
    "    ],\n",
    "    # Phoreal settings for processing photon data\n",
    "    \"phoreal\": {\n",
    "        \"binsize\": 1.0,          # Bin size for photon aggregation (e.g., 1 meter)\n",
    "        \"geoloc\": \"mean\",        # Method for geolocation aggregation (e.g., mean)\n",
    "        \"use_abs_h\": True,       # Whether to use absolute heights in calculations\n",
    "        \"send_waveform\": True    # Whether to include waveform data in the output\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458182da-7db9-4bdf-9a31-816446307d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.3 s, sys: 1.42 s, total: 45.8 s\n",
      "Wall time: 47.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Retrieve ATL03 data, retaining the extent ID\n",
    "atl03_data = icesat2.atl03sp(parms, keep_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27cdfd2-009b-4246-8ccb-ceca63507f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cycle', 'rgt', 'track', 'sc_orient', 'solar_elevation', 'segment_dist', 'segment_id', 'background_rate', 'extent_id', 'pair', 'region', 'atl08_class', 'atl03_cnf', 'height', 'relief', 'yapc_score', 'y_atc', 'landcover', 'snowcover', 'quality_ph', 'x_atc', 'geometry', 'spot']\n"
     ]
    }
   ],
   "source": [
    "print (atl03_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "229fdf56-8631-44d0-a6f7-c5ffc25063ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Group by 'extent_id' and 'x_atc' to define each photon group\n",
    "def get_photons_per_set(df, distance_window=5):\n",
    "    \"\"\"\n",
    "    Group data by 'x_atc' distance window, creating sets based on distance (e.g., 5 meters per window) \n",
    "    and considering unique 'extent_id'.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(by=['extent_id', 'x_atc'])  # Ensure sorted by extent and along-track distance\n",
    "    df['distance_group'] = df.groupby('extent_id')['x_atc'].transform(lambda x: (x // distance_window).astype(int))\n",
    "    return df\n",
    "\n",
    "# Apply rolling window (moving average) for ground height data\n",
    "def moving_window_distance(df, window_size=3, min_photons=2):\n",
    "    \"\"\"\n",
    "    Calculate average ground height using a moving window on 'distance_group', ensuring enough ground photons.\n",
    "    \"\"\"\n",
    "    # Filter for ground photons\n",
    "    ground_df = df[df['atl08_class'] == 1].copy()  # Ground photons only\n",
    "\n",
    "    # Apply rolling average calculation for ground heights\n",
    "    ground_df['avg_ground_height'] = ground_df.groupby(['extent_id', 'distance_group'])['height'].transform(\n",
    "        lambda x: x.rolling(window=window_size, min_periods=min_photons).mean()\n",
    "    )\n",
    "\n",
    "    # Handle groups with insufficient photons\n",
    "    ground_df['avg_ground_height'] = ground_df.groupby(['extent_id', 'distance_group'])['height'].transform(\n",
    "        lambda x: np.nan if len(x) < min_photons else x.mean()\n",
    "    )\n",
    "\n",
    "    # Fill NaN values using forward and backward fills\n",
    "    ground_df['avg_ground_height'] = ground_df['avg_ground_height'].ffill().bfill()\n",
    "\n",
    "    # Merge back to full DataFrame\n",
    "    df = pd.merge(df, ground_df[['extent_id', 'distance_group', 'avg_ground_height']], \n",
    "                  on=['extent_id', 'distance_group'], how='left')\n",
    "    return df\n",
    "\n",
    "# Compute relative height by subtracting average ground height\n",
    "def calculate_relative_height(df):\n",
    "    \"\"\"\n",
    "    Subtract average ground height from each photon to compute relative height.\n",
    "    \"\"\"\n",
    "    df['relative_height'] = df['height'] - df['avg_ground_height']\n",
    "    df['relative_height'] = df['relative_height'].fillna(0)  # Replace NaNs with 0 for cleaner output\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d19a363-18b6-49a9-aed2-155684f6e50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Extents: 100%|██████████| 12784/12784 [03:01<00:00, 70.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min, sys: 1.55 s, total: 3min 2s\n",
      "Wall time: 3min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize an empty list to collect processed data\n",
    "processed_data_list = []\n",
    "\n",
    "for extent_id in tqdm(atl03_data['extent_id'].unique(), desc=\"Processing Extents\"):\n",
    "    extent_data = atl03_data[atl03_data['extent_id'] == extent_id]\n",
    "\n",
    "    # Process segment data\n",
    "    extent_data = get_photons_per_set(extent_data, distance_window=5)  # 5-meter window\n",
    "    extent_data = moving_window_distance(extent_data, window_size=3, min_photons=2)\n",
    "    extent_data = calculate_relative_height(extent_data)\n",
    "\n",
    "    # Append processed data to the list\n",
    "    processed_data_list.append(extent_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb5617e-a293-4b23-8127-18b224d635e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total extents: 772\n",
      "Extents with all NaN values in 'relative_height': 0\n",
      "Extents with some NaN values in 'relative_height': 0\n"
     ]
    }
   ],
   "source": [
    "# Group by extent_id and count the number of NaNs in 'relative_height'\n",
    "na_counts = processed_atl03_data.groupby('extent_id')['relative_height'].apply(lambda x: x.isna().sum())\n",
    "\n",
    "# Convert to a DataFrame for easier inspection\n",
    "na_df = na_counts.reset_index(name='na_count')\n",
    "\n",
    "# Count the total number of non-NaN values for each extent_id\n",
    "extent_counts = processed_atl03_data.groupby('extent_id')['relative_height'].count().reset_index(name='total_count')\n",
    "\n",
    "# Merge the two DataFrames to align the counts\n",
    "merged_df = pd.merge(na_df, extent_counts, on='extent_id')\n",
    "\n",
    "# Number of extents with all NaN values in 'relative_height'\n",
    "all_na_extents = merged_df[merged_df['na_count'] == merged_df['total_count']].shape[0]\n",
    "\n",
    "# Number of extents with some NaN values in 'relative_height'\n",
    "some_na_extents = merged_df[merged_df['na_count'] > 0].shape[0]\n",
    "\n",
    "# Total number of extents\n",
    "total_extents = merged_df.shape[0]\n",
    "\n",
    "# Display the counts\n",
    "print(f\"Total extents: {total_extents}\")\n",
    "print(f\"Extents with all NaN values in 'relative_height': {all_na_extents}\")\n",
    "print(f\"Extents with some NaN values in 'relative_height': {some_na_extents}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce98cc-574b-45c4-94ac-2e2c2f36d072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
